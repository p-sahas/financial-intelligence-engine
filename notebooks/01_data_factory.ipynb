{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "0c185bc7",
            "metadata": {},
            "source": [
                "# Part 1: The Data Factory\n",
                "\n",
                "## Objective\n",
                "Transform `2024_Annual_Report.pdf` into a fine-tuning dataset of Question/Answer pairs.\n",
                "\n",
                "## Workflow\n",
                "1. **Ingestion**: LlamaParse to markdown (and JSON).\n",
                "2. **Chunking**: 1500 chars (Save to JSON).\n",
                "3. **Generation**: \n",
                "    - LLM A: Generate 10 Questions (Hard Facts, Strategic, Creative)\n",
                "    - LLM B: Generate Answers based on chunks\n",
                "4. **Storage**: Split 80/20 train/test JSONL."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b253619",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import uuid\n",
                "import random\n",
                "import asyncio\n",
                "import nest_asyncio\n",
                "from pathlib import Path\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Apply nest_asyncio for LlamaParse async loops in Jupyter\n",
                "nest_asyncio.apply()\n",
                "\n",
                "# Add project root to path\n",
                "notebook_dir = Path.cwd()\n",
                "project_root = notebook_dir.parent if notebook_dir.name == \"notebooks\" else notebook_dir\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "from src.services.llm_services import (\n",
                "    load_config,\n",
                "    get_llm,\n",
                "    get_pdf_parser,\n",
                "    print_config_summary,\n",
                "    load_pdf_and_save,\n",
                ")\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import JsonOutputParser\n",
                "\n",
                "# Load Environment & Config\n",
                "load_dotenv()\n",
                "config = load_config(str(project_root / \"src/config/config.yaml\"))\n",
                "print_config_summary(config)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b40ab3ad",
            "metadata": {},
            "source": [
                "## 1. Ingestion (LlamaParse)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1201dbd0",
            "metadata": {},
            "outputs": [],
            "source": [
                "pdf_path = project_root / \"data/pdfs/2024_Annual_Report.pdf\"\n",
                "\n",
                "print(f\"Loading PDF from: {pdf_path}\")\n",
                "\n",
                "parser = get_pdf_parser(config)\n",
                "# load_pdf_and_save RETURNS THE FULL TEXT STRING\n",
                "# It also saves .md and .json files now\n",
                "full_text = load_pdf_and_save(\n",
                "    pdf_path=str(pdf_path),\n",
                "    parser=parser,\n",
                "    output_dir=str(project_root / \"data/interim\")\n",
                ")\n",
                "# No need to join! full_text is already a string.\n",
                "print(f\"Total Characters: {len(full_text)}\")\n",
                "print(\"Sample content:\")\n",
                "print(full_text[:500])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4d4fd01d",
            "metadata": {},
            "source": [
                "## 2. Chunking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cd92c6dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split into 1500 character chunks\n",
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1500,\n",
                "    chunk_overlap=150,\n",
                "    length_function=len,\n",
                "    is_separator_regex=False,\n",
                ")\n",
                "\n",
                "chunks = text_splitter.create_documents([full_text])\n",
                "\n",
                "# Assign IDs and Save Chunks\n",
                "chunk_data = []\n",
                "for chunk in chunks:\n",
                "    chunk.metadata[\"chunk_id\"] = str(uuid.uuid4())\n",
                "    chunk_data.append({\n",
                "        \"chunk_id\": chunk.metadata[\"chunk_id\"],\n",
                "        \"content\": chunk.page_content,\n",
                "        \"metadata\": chunk.metadata\n",
                "    })\n",
                "\n",
                "chunks_save_path = project_root / \"data/interim/chunks.json\"\n",
                "with open(chunks_save_path, \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(chunk_data, f, indent=2)\n",
                "\n",
                "print(f\"Created {len(chunks)} chunks and saved {len(chunk_data)} items to {chunks_save_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4613b533",
            "metadata": {},
            "source": [
                "## 3. The Generation Loop (Q/A Generation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "78ed187d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LLMs with Fallbacks and Task Specificity\n",
                "# Use specific config if available, otherwise fall back to global config\n",
                "question_config = config.get(\"question_generation\", config)\n",
                "answer_config = config.get(\"answer_generation\", config)\n",
                "\n",
                "print(f\"Question Geneation Model: {question_config.get('llm_model')}\")\n",
                "print(f\"Answer Generation Model: {answer_config.get('llm_model')}\")\n",
                "\n",
                "question_llm = get_llm(question_config)\n",
                "answer_llm = get_llm(answer_config)\n",
                "\n",
                "# --- PROMPT A: Question Generation ---\n",
                "question_gen_system = \"\"\"\n",
                "You are an expert financial analyst creating a fine-tuning dataset.\n",
                "Your task is to generate 10 diverse questions based STRICTLY on the provided text chunk.\n",
                "\n",
                "The questions must cover these three categories:\n",
                "1. **Hard Facts**: Specific numbers, dates, names, or metrics found in the text.\n",
                "2. **Strategic Summaries**: High-level strategic goals, risks, or performance overviews.\n",
                "3. **Stylistic/Creative**: Questions about the tone, style, or specific phrasing used.\n",
                "\n",
                "Output format must be a raw JSON list of strings, e.g.:\n",
                "[\"Question 1\", \"Question 2\", ...]\n",
                "\"\"\"\n",
                "\n",
                "question_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", question_gen_system),\n",
                "    (\"human\", \"Context Chunk:\\n{chunk}\\n\\nGenerate 10 questions:\")\n",
                "])\n",
                "\n",
                "# --- PROMPT B: Answer Generation ---\n",
                "answer_gen_system = \"\"\"\n",
                "You are an expert financial analyst.\n",
                "Answer the following question based STRICTLY and ONLY on the provided context chunk.\n",
                "If the answer is not in the chunk, say \"Information not found in context.\"\n",
                "Be concise and professional.\n",
                "\"\"\"\n",
                "\n",
                "answer_prompt = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", answer_gen_system),\n",
                "    (\"human\", \"Context Chunk:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\")\n",
                "])\n",
                "\n",
                "# Chains\n",
                "question_chain = question_prompt | question_llm | JsonOutputParser()\n",
                "answer_chain = answer_prompt | answer_llm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "18297fcc",
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "qa_dataset = []\n",
                "\n",
                "print(f\"Processing {len(chunks)} chunks...\")\n",
                "\n",
                "for i, chunk in enumerate(tqdm(chunks)):\n",
                "    chunk_text = chunk.page_content\n",
                "    chunk_id = chunk.metadata.get(\"chunk_id\", f\"chunk_{i}\")\n",
                "    \n",
                "    # 1. Generate Questions\n",
                "    try:\n",
                "        questions = question_chain.invoke({\"chunk\": chunk_text})\n",
                "        if not isinstance(questions, list):\n",
                "            # Try to parse if string\n",
                "            questions = json.loads(questions)\n",
                "    except Exception as e:\n",
                "        print(f\"Error generating questions for chunk {i}: {e}\")\n",
                "        continue\n",
                "        \n",
                "    # 2. Generate Answers for each question\n",
                "    for q in questions:\n",
                "        try:\n",
                "            answer_response = answer_chain.invoke({\n",
                "                \"context\": chunk_text,\n",
                "                \"question\": q\n",
                "            })\n",
                "            answer_text = answer_response.content\n",
                "            \n",
                "            qa_dataset.append({\n",
                "                \"chunk_id\": chunk_id,\n",
                "                \"question\": q,\n",
                "                \"answer\": answer_text,\n",
                "                \"context\": chunk_text\n",
                "            })\n",
                "        except Exception as e:\n",
                "            print(f\"Error generating answer for Q: {q[:20]}... : {e}\")\n",
                "\n",
                "# Shuffle the dataset to ensure random distribution\n",
                "random.shuffle(qa_dataset)\n",
                "\n",
                "# Split 80/20 train/test\n",
                "split_idx = int(len(qa_dataset) * 0.8)\n",
                "train_data = qa_dataset[:split_idx]\n",
                "test_data = qa_dataset[split_idx:]\n",
                "\n",
                "processed_dir = project_root / \"data/processed\"\n",
                "processed_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Helper to save JSONL\n",
                "def save_jsonl(data, filename):\n",
                "    path = processed_dir / filename\n",
                "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
                "        for entry in data:\n",
                "            json.dump(entry, f)\n",
                "            f.write(\"\\n\")\n",
                "    print(f\"Saved {len(data)} items to {path}\")\n",
                "\n",
                "save_jsonl(train_data, \"train.jsonl\")\n",
                "save_jsonl(test_data, \"golden_test_set.jsonl\")\n",
                "\n",
                "# Also save the full raw dataset for backup\n",
                "qa_save_path = processed_dir / \"qa_dataset_full.json\"\n",
                "with open(qa_save_path, \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(qa_dataset, f, indent=2)\n",
                "\n",
                "print(f\"Total items generated: {len(qa_dataset)}\")\n",
                "print(f\"Full dataset saved to {qa_save_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}