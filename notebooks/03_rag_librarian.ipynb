{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_cell"
            },
            "source": [
                "# Part 3: \"The Librarian\" (Advanced RAG System)\n",
                "\n",
                "**Objective:** Build a robust, non-parametric memory system to retrieve exact information from the 2024 Annual Report.\n",
                "\n",
                "**Key Architecture:**\n",
                "- **Vector DB:** Weaviate (Embedded)\n",
                "- **Retrieval:** Hybrid Search (Dense Vectors + BM25 Keyword Search)\n",
                "- **Refinement:** Reciprocal Rank Fusion (RRF) + Cross-Encoder Reranking\n",
                "- **Generator:** Llama-3-8B (or compatible LLM)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup_header"
            },
            "source": [
                "## 1. Setup & Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "# Install Weaviate and RAG dependencies\n",
                "# !pip install -q -U weaviate-client langchain-weaviate langchain-community sentence-transformers rank_bm25 python-dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "id": "imports"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Libraries loaded.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import weaviate\n",
                "from weaviate.embedded import EmbeddedOptions\n",
                "from sentence_transformers import CrossEncoder\n",
                "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
                "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
                "from langchain_core.documents import Document\n",
                "\n",
                "print(\"Libraries loaded.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "path_config_header"
            },
            "source": [
                "## 2. Path Configuration (Hybrid Local/Colab)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "id": "path_config"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Local Windows Environment Detected.\n",
                        "Found Data: C:/Development/financial-intelligence-engine\\data/interim\\chunks.json\n"
                    ]
                }
            ],
            "source": [
                "# 1. Define the User's Local Path\n",
                "USER_LOCAL_ROOT = r\"C:/Development/financial-intelligence-engine\"\n",
                "USER_LOCAL_DATA = os.path.join(USER_LOCAL_ROOT, \"data/interim\")\n",
                "\n",
                "# 2. Check Environment\n",
                "if os.path.exists(USER_LOCAL_ROOT):\n",
                "    # Running Locally (Windows/VS Code Local Kernel)\n",
                "    print(\"Local Windows Environment Detected.\")\n",
                "    DATA_PATH = USER_LOCAL_DATA\n",
                "    CHUNKS_PATH = os.path.join(DATA_PATH, \"chunks.json\")\n",
                "    \n",
                "elif 'google.colab' in sys.modules:\n",
                "    # Running in Colab\n",
                "    print(\"Google Colab Environment Detected.\")\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    \n",
                "    # Drive Path\n",
                "    DRIVE_ROOT = \"/content/drive/MyDrive/Financial_Intern_Project\"\n",
                "    DATA_PATH = f\"{DRIVE_ROOT}/data/interim\"\n",
                "    CHUNKS_PATH = f\"{DATA_PATH}/chunks.json\"\n",
                "    \n",
                "    os.makedirs(DATA_PATH, exist_ok=True)\n",
                "    print(f\"Google Drive Mounted.\\nExpected Data Path: {CHUNKS_PATH}\")\n",
                "    \n",
                "else:\n",
                "    # Generic Fallback\n",
                "    print(\"Generic Environment.\")\n",
                "    DATA_PATH = \"../data/interim\"\n",
                "    CHUNKS_PATH = \"../data/interim/chunks.json\"\n",
                "\n",
                "# Validation\n",
                "if not os.path.exists(CHUNKS_PATH):\n",
                "    print(f\"ERROR: 'chunks.json' not found at {CHUNKS_PATH}\")\n",
                "    print(\"Colab Users: Upload 'chunks.json' to MyDrive/Financial_Intern_Project/data/interim/\")\n",
                "    print(\"Local Users: Check your data generation step.\")\n",
                "else:\n",
                "    print(f\"Found Data: {CHUNKS_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "vector_db_header"
            },
            "source": [
                "## 3. Vector Database Setup (Weaviate)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "id": "weaviate_client"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Attempting to connect to Weaviate (v4)...\n",
                        "Trying Embedded...\n",
                        "Embedded failed (or not supported in this env): Windows is not supported with EmbeddedDB. Please upvote this feature request if you want\n",
                        "                 this: https://github.com/weaviate/weaviate/issues/3315\n",
                        "Attempting to connect to external instance...\n",
                        "Connected to Weaviate at http://localhost:8080\n",
                        "Client Ready.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\weaviate\\warnings.py:196: DeprecationWarning: Dep024: You are using the `vectorizer_config` argument in `collection.config.create()`, which is deprecated.\n",
                        "            Use the `vector_config` argument instead.\n",
                        "            \n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Schema 'FinancialReport' created/reset.\n"
                    ]
                }
            ],
            "source": [
                "# Weaviate v4 Connection Logic\n",
                "import weaviate\n",
                "import os\n",
                "from urllib.parse import urlparse\n",
                "from weaviate.classes.init import Auth\n",
                "import weaviate.classes.config as wn\n",
                "\n",
                "print(\"Attempting to connect to Weaviate (v4)...\")\n",
                "\n",
                "headers = {\n",
                "    \"X-HuggingFace-Api-Key\": os.getenv(\"HF_TOKEN\", \"\")\n",
                "}\n",
                "\n",
                "client = None\n",
                "\n",
                "# 1. Try Embedded\n",
                "try:\n",
                "    print(\"Trying Embedded...\")\n",
                "    client = weaviate.connect_to_embedded(\n",
                "        headers=headers,\n",
                "    )\n",
                "    print(\"Weaviate Embedded Started!\")\n",
                "except Exception as e:\n",
                "    print(f\"Embedded failed (or not supported in this env): {e}\")\n",
                "    client = None\n",
                "\n",
                "# 2. Fallback to Local/External if Embedded failed\n",
                "if not client:\n",
                "    print(\"Attempting to connect to external instance...\")\n",
                "    wcd_url = os.getenv(\"WEAVIATE_URL\", \"http://localhost:8080\")\n",
                "    wcd_api_key = os.getenv(\"WEAVIATE_API_KEY\", \"\")\n",
                "\n",
                "    try:\n",
                "        parsed = urlparse(wcd_url)\n",
                "        host = parsed.hostname or \"localhost\"\n",
                "        port = parsed.port or (443 if parsed.scheme == 'https' else 80)\n",
                "        secure = parsed.scheme == 'https'\n",
                "        \n",
                "        auth_config = Auth.api_key(wcd_api_key) if wcd_api_key else None\n",
                "        \n",
                "        if \"localhost\" in host or \"127.0.0.1\" in host:\n",
                "             client = weaviate.connect_to_local(port=port, headers=headers)\n",
                "        else:\n",
                "             client = weaviate.connect_to_custom(\n",
                "                http_host=host,\n",
                "                http_port=port,\n",
                "                http_secure=secure,\n",
                "                headers=headers,\n",
                "                auth_credentials=auth_config\n",
                "             )\n",
                "        print(f\"Connected to Weaviate at {wcd_url}\")\n",
                "    except Exception as e_ext:\n",
                "        print(f\"CRITICAL: Could not connect to Weaviate. Error: {e_ext}\")\n",
                "        raise e_ext\n",
                "\n",
                "if client and client.is_ready():\n",
                "    print(\"Client Ready.\")\n",
                "    # Define Schema\n",
                "    class_name = \"FinancialReport\"\n",
                "    if client.collections.exists(class_name):\n",
                "        client.collections.delete(class_name)\n",
                "    \n",
                "    client.collections.create(\n",
                "        name=class_name,\n",
                "        vectorizer_config=wn.Configure.Vectorizer.none(),\n",
                "        properties=[\n",
                "            wn.Property(name=\"text\", data_type=wn.DataType.TEXT),\n",
                "            wn.Property(name=\"source\", data_type=wn.DataType.TEXT),\n",
                "            wn.Property(name=\"chunk_id\", data_type=wn.DataType.INT),\n",
                "        ]\n",
                "    )\n",
                "    print(f\"Schema '{class_name}' created/reset.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ingest_header"
            },
            "source": [
                "## 4. Data Ingestion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "id": "ingest_data"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Sahas Induwara\\AppData\\Local\\Temp\\ipykernel_9772\\2064334838.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
                        "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
                        "c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded Embeddings: all-MiniLM-L6-v2\n",
                        "Loading 654 chunks to Weaviate (Collection: FinancialReport)...\n",
                        "Ingestion Complete. Total Objects: 0\n"
                    ]
                }
            ],
            "source": [
                "# Load Embeddings Model (Local)\n",
                "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
                "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
                "print(f\"Loaded Embeddings: {embedding_model_name}\")\n",
                "\n",
                "# Load Data\n",
                "with open(CHUNKS_PATH, 'r') as f:\n",
                "    raw_chunks = json.load(f)\n",
                "\n",
                "collection = client.collections.get(class_name)\n",
                "print(f\"Loading {len(raw_chunks)} chunks to Weaviate (Collection: {class_name})...\")\n",
                "\n",
                "with collection.batch.fixed_size(batch_size=100) as batch:\n",
                "    for i, chunk in enumerate(raw_chunks):\n",
                "        text = chunk.get(\"chunk_content\", \"\")\n",
                "        source = chunk.get(\"source\", \"Unknown\")\n",
                "        if not text: continue\n",
                "        \n",
                "        vector = embeddings.embed_query(text)\n",
                "        \n",
                "        batch.add_object(\n",
                "            properties={\n",
                "                \"text\": text,\n",
                "                \"source\": source,\n",
                "                \"chunk_id\": i\n",
                "            },\n",
                "            vector=vector\n",
                "        )\n",
                "        if i % 100 == 0:\n",
                "            print(f\"Imported {i} chunks...\")\n",
                "\n",
                "print(f\"Ingestion Complete. Total Objects: {collection.aggregate.over_all(total_count=True).total_count}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "search_header"
            },
            "source": [
                "## 5. Retrieval Strategies (Hybrid + RRF)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "id": "hybrid_search_func"
            },
            "outputs": [],
            "source": [
                "import weaviate.classes.query as wq\n",
                "\n",
                "def hybrid_search(query, limit=20):\n",
                "    \"\"\"\n",
                "    Performs Hybrid Search: Dense Vector + BM25 (Keyword)\n",
                "    \"\"\"\n",
                "    collection = client.collections.get(class_name)\n",
                "    query_vector = embeddings.embed_query(query)\n",
                "    \n",
                "    response = collection.query.hybrid(\n",
                "        query=query,\n",
                "        vector=query_vector,\n",
                "        alpha=0.5,\n",
                "        limit=limit,\n",
                "        fusion_type=wq.HybridFusion.RELATIVE_SCORE,\n",
                "        return_metadata=wq.MetadataQuery(score=True)\n",
                "    )\n",
                "    \n",
                "    results = []\n",
                "    for o in response.objects:\n",
                "        res = o.properties\n",
                "        # Add score for compatibility with reranker if needed\n",
                "        if o.metadata and o.metadata.score is not None:\n",
                "            res['_additional'] = {'score': o.metadata.score}\n",
                "        results.append(res)\n",
                "        \n",
                "    return results\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rerank_header"
            },
            "source": [
                "## 6. Refinement: Cross-Encoder Reranking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "id": "reranker_setup"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n"
                    ]
                }
            ],
            "source": [
                "# Load Cross-Encoder (Reranker)\n",
                "rerank_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
                "reranker = CrossEncoder(rerank_model_name)\n",
                "print(f\"Loaded Reranker: {rerank_model_name}\")\n",
                "\n",
                "def rerank_results(query, retrieved_docs, top_k=5):\n",
                "    \"\"\"\n",
                "    Reranks the hybrid search results using a Cross-Encoder\n",
                "    \"\"\"\n",
                "    if not retrieved_docs:\n",
                "        return []\n",
                "        \n",
                "    # Prepare pairs for Cross-Encoder\n",
                "    chunk_texts = [doc['text'] for doc in retrieved_docs]\n",
                "    pairs = [[query, text] for text in chunk_texts]\n",
                "    \n",
                "    # Score pairs\n",
                "    scores = reranker.predict(pairs)\n",
                "    \n",
                "    # Attach scores and sort\n",
                "    ranked_results = []\n",
                "    for doc, score in zip(retrieved_docs, scores):\n",
                "        doc['rerank_score'] = score\n",
                "        ranked_results.append(doc)\n",
                "        \n",
                "    # Sort by score descending\n",
                "    ranked_results = sorted(ranked_results, key=lambda x: x['rerank_score'], reverse=True)\n",
                "    \n",
                "    return ranked_results[:top_k]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "librarian_header"
            },
            "source": [
                "## 7. The Librarian (Inference)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "librarian_func"
            },
            "outputs": [],
            "source": [
                "# Setup LLM (Can use the Fine-tuned one if loaded, or a lightweight one)\n",
                "\n",
                "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
                "import torch\n",
                "\n",
                "# Load Model (Optional: Reuse Part 2 logic/paths if unified)\n",
                "model_id = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
                "try:\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_id,\n",
                "        torch_dtype=torch.float16,\n",
                "        device_map=\"auto\",\n",
                "        load_in_4bit=True\n",
                "    )\n",
                "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
                "    print(\"LLM Loaded for Librarian\")\n",
                "except Exception as e:\n",
                "    print(f\"LLM Load Warning: {e}. Ensure you have GPU/Space. You can also mock this for retrieval testing.\")\n",
                "\n",
                "def query_librarian(question):\n",
                "    # 1. Retrieval\n",
                "    raw_results = hybrid_search(question, limit=20)\n",
                "    \n",
                "    # 2. Refinement\n",
                "    refined_results = rerank_results(question, raw_results, top_k=5)\n",
                "    \n",
                "    # 3. Context Construction\n",
                "    context_text = \"\\n---\\n\".join([doc['text'] for doc in refined_results])\n",
                "    \n",
                "    # 4. Generation\n",
                "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "\n",
                "You are The Librarian, a precise financial assistant. Answer the question specifically using ONLY the provided context below. If the answer is not in the context, say 'Information not found'.\n",
                "\n",
                "Context:\n",
                "{context_text}\n",
                "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "\n",
                "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
                "    \n",
                "    try:\n",
                "        outputs = pipe(\n",
                "            prompt,\n",
                "            max_new_tokens=256,\n",
                "            do_sample=True,\n",
                "            temperature=0.1,\n",
                "            top_p=0.9\n",
                "        )\n",
                "        answer = outputs[0][\"generated_text\"].split(\"assistant\")[-1].strip()\n",
                "    except NameError:\n",
                "        answer = \"[LLM Not Loaded - Retrieval Only Mode]\"\n",
                "        \n",
                "    return {\n",
                "        \"answer\": answer,\n",
                "        \"context\": refined_results\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eval_header"
            },
            "source": [
                "## 8. Evaluation & Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run_eval"
            },
            "outputs": [],
            "source": [
                "question = \"What is the total revenue for 2024?\"\n",
                "result = query_librarian(question)\n",
                "\n",
                "print(f\"Question: {question}\")\n",
                "print(f\"Answer: {result['answer']}\")\n",
                "print(\"\\nSource Contexts:\")\n",
                "for i, doc in enumerate(result['context']):\n",
                "    print(f\"[{i+1}] (Score: {doc['rerank_score']:.4f}) {doc['text'][:150]}...\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "sahas",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
