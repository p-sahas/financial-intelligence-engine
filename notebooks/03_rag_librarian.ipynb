{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9f3e74",
   "metadata": {},
   "source": [
    "# 03. RAG Librarian (Advanced)\n",
    "\n",
    "This notebook sets up the Knowledge Base (Weaviate), performs Hybrid Search (Dense + Sparse), Reranking, and Evaluates the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882f9366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -U weaviate-client langchain-weaviate langchain-community sentence-transformers rank_bm25 tiktoken\n",
    "!pip install -q -U langchain-openai langchain-groq langchain-google-genai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbeaf053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Config loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "import weaviate.classes.config as wn\n",
    "import weaviate.classes.query as wq\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Add parent directory to path\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent if notebook_dir.name == \"notebooks\" else notebook_dir\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.services.llm_services import get_llm, load_config\n",
    "from src.utils.cost_tracker import get_accurate_cost\n",
    "\n",
    "load_dotenv()\n",
    "config = load_config(os.path.join(project_root, \"src/config/config.yaml\"))\n",
    "print(\"Config loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cefb4a",
   "metadata": {},
   "source": [
    "## 1. Connect to Weaviate (v4)\n",
    "\n",
    "Trying Embedded first, falling back to Local Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6928a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to Weaviate (v4)...\n",
      "1. Trying Embedded...\n",
      "   Embedded failed/skipped: Windows is not supported with EmbeddedDB. Please upvote this feature request if you want\n",
      "                 this: https://github.com/weaviate/weaviate/issues/3315\n",
      "2. Trying Local Docker (localhost:8080)...\n",
      " Connected to Local Docker!\n",
      "Client Ready.\n"
     ]
    }
   ],
   "source": [
    "print(\"Attempting to connect to Weaviate (v4)...\")\n",
    "\n",
    "headers = {\n",
    "    \"X-HuggingFace-Api-Key\": os.getenv(\"HF_TOKEN\", \"\")\n",
    "}\n",
    "\n",
    "client = None\n",
    "\n",
    "# 1. Try Embedded\n",
    "try:\n",
    "    print(\"1. Trying Embedded...\")\n",
    "    client = weaviate.connect_to_embedded(headers=headers)\n",
    "    print(\" Weaviate Embedded Started!\")\n",
    "except Exception as e:\n",
    "    print(f\"   Embedded failed/skipped: {e}\")\n",
    "    client = None\n",
    "\n",
    "# 2. Try Local Docker (Preferred for Windows users with Docker)\n",
    "if not client:\n",
    "    print(\"2. Trying Local Docker (localhost:8080)...\")\n",
    "    try:\n",
    "        client = weaviate.connect_to_local(\n",
    "            port=8080,\n",
    "            grpc_port=50051,\n",
    "            headers=headers\n",
    "        )\n",
    "        print(\" Connected to Local Docker!\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Local Docker failed: {e}\")\n",
    "\n",
    "# 3. Try Cloud (WCS) from .env\n",
    "if not client:\n",
    "    wcd_url = os.getenv(\"WEAVIATE_URL\")\n",
    "    wcd_api_key = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "    if wcd_url and wcd_api_key:\n",
    "        print(f\"3. Trying Weaviate Cloud: {wcd_url}...\")\n",
    "        try:\n",
    "            client = weaviate.connect_to_wcs(\n",
    "                cluster_url=wcd_url,\n",
    "                auth_credentials=Auth.api_key(wcd_api_key),\n",
    "                headers=headers\n",
    "            )\n",
    "            print(\" Connected to Weaviate Cloud!\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Cloud connection failed: {e}\")\n",
    "\n",
    "if not client or not client.is_ready():\n",
    "    raise ConnectionError(\"CRITICAL: Could not connect to any Weaviate instance (Embedded, Docker, or Cloud). Please check your setup.\")\n",
    "\n",
    "print(\"Client Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff6384",
   "metadata": {},
   "source": [
    "## 2. Prepare Schema & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4dfdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema 'FinancialReport' created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\weaviate\\warnings.py:196: DeprecationWarning: Dep024: You are using the `vectorizer_config` argument in `collection.config.create()`, which is deprecated.\n",
      "            Use the `vector_config` argument instead.\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Schema\n",
    "class_name = \"FinancialReport\"\n",
    "\n",
    "if client.collections.exists(class_name):\n",
    "    client.collections.delete(class_name)\n",
    "\n",
    "# Create Collection with properties\n",
    "client.collections.create(\n",
    "    name=class_name,\n",
    "    vectorizer_config=wn.Configure.Vectorizer.none(), # We embed manually\n",
    "    properties=[\n",
    "        wn.Property(name=\"text\", data_type=wn.DataType.TEXT),\n",
    "        wn.Property(name=\"source\", data_type=wn.DataType.TEXT),\n",
    "        wn.Property(name=\"chunk_id\", data_type=wn.DataType.INT),\n",
    "        # Add other metadata fields as needed\n",
    "        wn.Property(name=\"category\", data_type=wn.DataType.TEXT),\n",
    "        wn.Property(name=\"page_number\", data_type=wn.DataType.INT),\n",
    "    ]\n",
    ")\n",
    "print(f\"Schema '{class_name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "679604c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahas Induwara\\AppData\\Local\\Temp\\ipykernel_4324\\2058290492.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
      "c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Embeddings: sentence-transformers/all-MiniLM-L6-v2\n",
      "Loading 3000 chunks...\n",
      "Imported 0...\n",
      "Imported 100...\n",
      "Imported 200...\n",
      "Imported 300...\n",
      "Imported 400...\n",
      "Imported 500...\n",
      "Imported 600...\n",
      "Imported 700...\n",
      "Imported 800...\n",
      "Imported 900...\n",
      "Imported 1000...\n",
      "Imported 1100...\n",
      "Imported 1200...\n",
      "Imported 1300...\n",
      "Imported 1400...\n",
      "Imported 1500...\n",
      "Imported 1600...\n",
      "Imported 1700...\n",
      "Imported 1800...\n",
      "Imported 1900...\n",
      "Imported 2000...\n",
      "Imported 2100...\n",
      "Imported 2200...\n",
      "Imported 2300...\n",
      "Imported 2400...\n",
      "Imported 2500...\n",
      "Imported 2600...\n",
      "Imported 2700...\n",
      "Imported 2800...\n",
      "Imported 2900...\n",
      "Ingestion Complete. Total: 3000\n"
     ]
    }
   ],
   "source": [
    "# Load Embeddings Model\n",
    "embedding_model_name = config.get('text_emb_model', \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "print(f\"Loaded Embeddings: {embedding_model_name}\")\n",
    "\n",
    "# Load Data from Processed Directory\n",
    "CHUNKS_PATH = os.path.join(project_root, config.get('chunks_json_path', 'data/processed/qa_dataset_full.json'))\n",
    "\n",
    "with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:\n",
    "    raw_chunks = json.load(f)\n",
    "\n",
    "collection = client.collections.get(class_name)\n",
    "print(f\"Loading {len(raw_chunks)} chunks...\")\n",
    "\n",
    "with collection.batch.fixed_size(batch_size=100) as batch:\n",
    "    for i, chunk in enumerate(raw_chunks):\n",
    "        # Handle both flat list of dicts or list of objects\n",
    "        # Fix: Look for 'context' (from 01_data_factory) as well as 'chunk_content'/'page_content'\n",
    "        text = chunk.get(\"context\", chunk.get(\"chunk_content\", chunk.get(\"page_content\", \"\")))\n",
    "        \n",
    "        # Extract metadata if available, otherwise default\n",
    "        meta = chunk.get(\"metadata\", {})\n",
    "        source_val = meta.get(\"source\", chunk.get(\"source\", \"Unknown\"))\n",
    "        \n",
    "        if not text: \n",
    "            if i < 3: print(f\"DEBUG: Skipping empty chunk {i}. Keys: {list(chunk.keys())}\")\n",
    "            continue\n",
    "        \n",
    "        vector = embeddings.embed_query(text)\n",
    "        \n",
    "        batch.add_object(\n",
    "            properties={\n",
    "                \"text\": text,\n",
    "                \"source\": source_val,\n",
    "                \"chunk_id\": i,\n",
    "                \"category\": meta.get(\"category\", \"Text\"),\n",
    "                \"page_number\": meta.get(\"page_number\", 0)\n",
    "            },\n",
    "            vector=vector\n",
    "        )\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Imported {i}...\")\n",
    "\n",
    "print(f\"Ingestion Complete. Total: {collection.aggregate.over_all(total_count=True).total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761348b",
   "metadata": {},
   "source": [
    "## 3. Retrieval Strategy: Hybrid + Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10354573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n"
     ]
    }
   ],
   "source": [
    "# 1. Hybrid Search Function (Weaviate)\n",
    "def hybrid_search(query, limit=20):\n",
    "    collection = client.collections.get(class_name)\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    \n",
    "    response = collection.query.hybrid(\n",
    "        query=query,\n",
    "        vector=query_vector,\n",
    "        alpha=0.5, # 0.5 = Equal weight to Dense and Sparse\n",
    "        limit=limit,\n",
    "        fusion_type=wq.HybridFusion.RELATIVE_SCORE,\n",
    "        return_metadata=wq.MetadataQuery(score=True)\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for o in response.objects:\n",
    "        res = o.properties\n",
    "        res['score'] = o.metadata.score\n",
    "        results.append(res)\n",
    "    return results\n",
    "\n",
    "# 2. Reranker Setup\n",
    "rerank_model_name = config.get('rerank_settings', {}).get('model', \"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "reranker = CrossEncoder(rerank_model_name)\n",
    "print(f\"Loaded Reranker: {rerank_model_name}\")\n",
    "\n",
    "# 3. Rerank Function\n",
    "def rerank_results(query, retrieved_docs, top_k=5):\n",
    "    if not retrieved_docs: return []\n",
    "    \n",
    "    pairs = [[query, doc['text']] for doc in retrieved_docs]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        doc['rerank_score'] = float(scores[i])\n",
    "    \n",
    "    # Sort by rerank score\n",
    "    return sorted(retrieved_docs, key=lambda x: x['rerank_score'], reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f538e",
   "metadata": {},
   "source": [
    "## 4. Generation & Evaluation Loop\n",
    "\n",
    "Using the Golden Test Set to evaluate performance and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b240de9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Initialized: groq\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Initialize LLM\n",
    "llm = get_llm(config)\n",
    "print(f\"LLM Initialized: {config['llm_provider']}\")\n",
    "\n",
    "# Prompt Template\n",
    "template = \"\"\"You are a financial analyst. Answer based ONLY on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d['text'] for d in docs])\n",
    "\n",
    "def query_system(question):\n",
    "    # 1. Retrieve & Rerank\n",
    "    initial_docs = hybrid_search(question, limit=20)\n",
    "    top_docs = rerank_results(question, initial_docs, top_k=5)\n",
    "    \n",
    "    # 2. Generate\n",
    "    context = format_docs(top_docs)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    return answer, top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fcf66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation on 5 questions (preview)...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     latency = time.time() - start\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Calculate Cost\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     cost = \u001b[43mget_accurate_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllm_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     results.append({\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: q,\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgenerated\u001b[39m\u001b[33m\"\u001b[39m: gen_answer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcost\u001b[39m\u001b[33m\"\u001b[39m: cost\n\u001b[32m     35\u001b[39m     })\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluation Complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Development\\financial-intelligence-engine\\src\\utils\\cost_tracker.py:41\u001b[39m, in \u001b[36mget_accurate_cost\u001b[39m\u001b[34m(sources, query, answer, prompt_template, model_name)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sources:\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Handle both dict and object formats\u001b[39;00m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         content = \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdoc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s.get(\u001b[33m'\u001b[39m\u001b[33mdoc\u001b[39m\u001b[33m'\u001b[39m), \u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m s.get(\u001b[33m'\u001b[39m\u001b[33mpage_content\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     43\u001b[39m         content = s.page_content \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(s, \u001b[33m'\u001b[39m\u001b[33mpage_content\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(s)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Load Golden Test Set\n",
    "TEST_SET_PATH = os.path.join(project_root, config.get('golden_test_set_path', 'data/processed/golden_test_set.jsonl'))\n",
    "test_data = []\n",
    "\n",
    "if os.path.exists(TEST_SET_PATH):\n",
    "    with open(TEST_SET_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            test_data.append(json.loads(line))\n",
    "else:\n",
    "    print(\"Test set not found. Skipping evaluation.\")\n",
    "\n",
    "# Run Evaluation\n",
    "results = []\n",
    "print(f\"Running Evaluation on {len(test_data[:5])} questions (preview)...\")\n",
    "\n",
    "for item in test_data[:5]: # Cap at 5 for quick test\n",
    "    q = item['question']\n",
    "    actual = item['answer']\n",
    "    \n",
    "    start = time.time()\n",
    "    gen_answer, sources = query_system(q)\n",
    "    latency = time.time() - start\n",
    "    \n",
    "    # Calculate Cost\n",
    "    cost = get_accurate_cost(sources, q, gen_answer, prompt, model_name=config.get('llm_model', 'gpt-4o-mini'))\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"generated\": gen_answer,\n",
    "        \"actual\": actual,\n",
    "        \"latency\": latency,\n",
    "        \"cost\": cost\n",
    "    })\n",
    "    \n",
    "    \n",
    "print(\"Evaluation Complete.\")\n",
    "print(json.dumps(results[0], indent=2)) # Show sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e7bc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to c:\\Development\\financial-intelligence-engine\\data/results/rag_evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save Results\n",
    "RESULTS_PATH = os.path.join(project_root, config.get('eval_results_path', 'data/results/rag_evaluation_results.json'))\n",
    "os.makedirs(os.path.dirname(RESULTS_PATH), exist_ok=True)\n",
    "\n",
    "with open(RESULTS_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"Results saved to {RESULTS_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
