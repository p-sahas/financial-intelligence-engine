{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_cell"
            },
            "source": [
                "# Part 2: \"The Intern\" (Fine-Tuning) - Refined Edition\n",
                "\n",
                "**Objective:** Fine-tune a Llama-3-8B model on the financial dataset to learn the strategy and tone of the 2024 Annual Report.\n",
                "\n",
                "**Key Features:**\n",
                "- **Reproducibility:** Strict random seed setting.\n",
                "- **Visualization:** Training loss plots and Base vs. Tuned comparisons.\n",
                "- **Extended Evaluation:** Testing on `golden_test_set.jsonl` and saving results.\n",
                "- **Optimization:** 4-bit quantization and LoRA optimization."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup_header"
            },
            "source": [
                "## 1. Setup & Dependencies\n",
                "\n",
                "**IMPORTANT:** If you see an error like `AttributeError: partially initialized module 'torchvision'` or `ModuleNotFoundError: PreTrainedModel`, you **MUST**:\n",
                "1. Run the cell below.\n",
                "2. **Restart the Runtime** (Runtime -> Restart session).\n",
                "3. Run the valid imports cell (skip the install cell if you just ran it)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found existing installation: torchvision 0.20.1+cu121\n",
                        "Uninstalling torchvision-0.20.1+cu121:\n",
                        "  Successfully uninstalled torchvision-0.20.1+cu121\n",
                        "Found existing installation: torch 2.5.1+cu121\n",
                        "Uninstalling torch-2.5.1+cu121:\n",
                        "  Successfully uninstalled torch-2.5.1+cu121\n",
                        "Found existing installation: torchaudio 2.5.1+cu121\n",
                        "Uninstalling torchaudio-2.5.1+cu121:\n",
                        "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
                        "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
                        "Collecting torch\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
                        "Collecting torchvision\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
                        "Collecting torchaudio\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
                        "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
                        "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
                        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch) (9.1.0.70)\n",
                        "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.3.1)\n",
                        "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch) (11.0.2.54)\n",
                        "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.2.106)\n",
                        "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch) (11.4.5.107)\n",
                        "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.0.106)\n",
                        "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.21.5)\n",
                        "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.0)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
                        "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
                        "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
                        "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
                        "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
                        "Installing collected packages: torch, torchvision, torchaudio\n",
                        "Successfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.colab-display-data+json": {
                            "id": "0f552d99fd8046738b74adc27bb80828",
                            "pip_warning": {
                                "packages": [
                                    "torch",
                                    "torchgen",
                                    "torchvision"
                                ]
                            }
                        }
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Installation complete. PLEASE RESTART RUNTIME now if this is the first run.\n"
                    ]
                }
            ],
            "source": [
                "# 1. Uninstall potentially conflicting versions first\n",
                "!pip uninstall -y torchvision torch torchaudio\n",
                "\n",
                "# 2. Install compatible Torch + Torchvision\n",
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
                "\n",
                "# 3. Install other dependencies (including python-dotenv for local env)\n",
                "!pip install -q -U transformers peft trl bitsandbytes accelerate datasets scipy matplotlib python-dotenv\n",
                "\n",
                "print(\"Installation complete. PLEASE RESTART RUNTIME now if this is the first run.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "id": "imports_and_env"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch Version: 2.5.1+cu121\n",
                        "CUDA Available: True\n",
                        "Device: Tesla T4\n",
                        "VRAM: 14.56 GB\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import os\n",
                "import random\n",
                "import numpy as np\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                "    logging,\n",
                ")\n",
                "from peft import LoraConfig, PeftModel\n",
                "from trl import SFTTrainer\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Environment Check\n",
                "print(f\"PyTorch Version: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "seeds_header"
            },
            "source": [
                "## 2. strict Reproducibility (Seeds)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "id": "set_seeds"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ Seeds set to 42 for reproducibility\n",
                        "⚠️ Note: Full determinism on GPU is not guaranteed due to non-deterministic operations\n"
                    ]
                }
            ],
            "source": [
                "SEED = 42\n",
                "\n",
                "# Set environment variable for Python hash seed\n",
                "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
                "\n",
                "# Set seeds\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed(SEED)\n",
                "    torch.cuda.manual_seed_all(SEED)\n",
                "    # Note: These settings may impact performance\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "\n",
                "print(f\"✅ Seeds set to {SEED} for reproducibility\")\n",
                "print(\"⚠️ Note: Full determinism on GPU is not guaranteed due to non-deterministic operations\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "config_header"
            },
            "source": [
                "## 3. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "id": "config_cell"
            },
            "outputs": [],
            "source": [
                "CONFIG = {\n",
                "    # Model\n",
                "    \"model_id\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
                "    \"new_model_name\": \"Llama-3-8b-Financial-Intern\",\n",
                "    \"hf_username\": \"p-sahas\", \n",
                "    \n",
                "    # LoRA\n",
                "    \"lora_r\": 64,\n",
                "    \"lora_alpha\": 16,\n",
                "    \"lora_dropout\": 0.1,\n",
                "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    \n",
                "    # Training\n",
                "    \"batch_size\": 4,\n",
                "    \"grad_accumulation\": 1,\n",
                "    \"learning_rate\": 2e-4,\n",
                "    \"num_epochs\": 1,\n",
                "    \"max_steps\": 150,\n",
                "    \"logging_steps\": 10,\n",
                "    \"save_steps\": 50,\n",
                "    \"max_seq_length\": 512,\n",
                "    \"output_dir\": \"./results\",\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "auth_header"
            },
            "source": [
                "## 4. Hugging Face Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "id": "hf_auth"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Colab secret fetch skipped: Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.\n",
                        "Automatic login failed: HF_TOKEN not found in environment variable or Colab secrets.\n",
                        "Falling back to manual login...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
                        "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
                        "You are not authenticated with the Hugging Face Hub in this notebook.\n",
                        "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ca232bfb56724aa9919d144ff344a6be",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from huggingface_hub import login\n",
                "import os\n",
                "\n",
                "# Attempt to load .env file if available (local usage)\n",
                "try:\n",
                "    from dotenv import load_dotenv\n",
                "    load_dotenv()\n",
                "except ImportError:\n",
                "    print(\"python-dotenv not installed, skipping .env loading (pip install python-dotenv)\")\n",
                "\n",
                "try:\n",
                "    # 1. Try Environment Variable (from .env or system)\n",
                "    token = os.getenv(\"HF_TOKEN\")\n",
                "    \n",
                "    # 2. If missing, Try Colab Secrets\n",
                "    if not token:\n",
                "        try:\n",
                "            from google.colab import userdata\n",
                "            token = userdata.get('HF_TOKEN')\n",
                "        except ImportError:\n",
                "            pass # Not in Colab\n",
                "        except Exception as e:\n",
                "            print(f\"Colab secret fetch skipped: {e}\")\n",
                "\n",
                "    # 3. Login or Fallback\n",
                "    if token:\n",
                "        print(f\"Logging in with token: {token[:4]}...{token[-4:]}\")\n",
                "        login(token)\n",
                "    else:\n",
                "        raise ValueError(\"HF_TOKEN not found in environment variable or Colab secrets.\")\n",
                "\n",
                "except Exception as e:\n",
                "    # Fallback to interactive login\n",
                "    print(f\"Automatic login failed: {e}\")\n",
                "    print(\"Falling back to manual login...\")\n",
                "    login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "data_header"
            },
            "source": [
                "## 5. Data Loading & Formatting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "id": "data_load"
            },
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "Unable to find '/content/../data/train.jsonl'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-2175911402.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/train.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mformat_instruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1488\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1489\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1134\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         ).get_module()\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;31m# Try locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n\u001b[0;32m--> 528\u001b[0;31m         data_files = DataFilesDict.from_patterns(\n\u001b[0m\u001b[1;32m    529\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFilesList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 else DataFilesList.from_patterns(\n\u001b[0m\u001b[1;32m    709\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 data_files.extend(\n\u001b[0;32m--> 601\u001b[0;31m                     resolve_pattern(\n\u001b[0m\u001b[1;32m    602\u001b[0m                         \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                         \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mallowed_extensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" with any supported extension {list(allowed_extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/content/../data/train.jsonl'"
                    ]
                }
            ],
            "source": [
                "dataset_name = \"../data/train.jsonl\"\n",
                "train_dataset = load_dataset(\"json\", data_files=dataset_name, split=\"train\")\n",
                "\n",
                "def format_instruction(sample):\n",
                "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "\n",
                "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on context.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "\n",
                "{sample['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
                "\n",
                "{sample['answer']}<|eot_id|>\"\"\"\n",
                "\n",
                "print(f\"Loaded {len(train_dataset)} training examples.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "model_header"
            },
            "source": [
                "## 6. Model Loading (4-Bit)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model_load"
            },
            "outputs": [],
            "source": [
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    CONFIG[\"model_id\"],\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    use_cache=False,\n",
                ")\n",
                "model.config.pretraining_tp = 1\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_id\"], trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "train_header"
            },
            "source": [
                "## 7. Training & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training"
            },
            "outputs": [],
            "source": [
                "peft_config = LoraConfig(\n",
                "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
                "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
                "    r=CONFIG[\"lora_r\"],\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    target_modules=CONFIG[\"target_modules\"]\n",
                ")\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=CONFIG[\"output_dir\"],\n",
                "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
                "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
                "    gradient_accumulation_steps=CONFIG[\"grad_accumulation\"],\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    save_steps=CONFIG[\"save_steps\"],\n",
                "    logging_steps=CONFIG[\"logging_steps\"],\n",
                "    learning_rate=CONFIG[\"learning_rate\"],\n",
                "    weight_decay=0.001,\n",
                "    fp16=True,\n",
                "    bf16=False,\n",
                "    max_grad_norm=0.3,\n",
                "    max_steps=CONFIG[\"max_steps\"],\n",
                "    warmup_ratio=0.03,\n",
                "    group_by_length=True,\n",
                "    lr_scheduler_type=\"constant\",\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=train_dataset,\n",
                "    peft_config=peft_config,\n",
                "    formatting_func=format_instruction,\n",
                "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "print(\"Starting training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "loss_plot"
            },
            "outputs": [],
            "source": [
                "# Visualize Training Loss\n",
                "history = trainer.state.log_history\n",
                "steps = []\n",
                "losses = []\n",
                "\n",
                "for entry in history:\n",
                "    if 'loss' in entry:\n",
                "        steps.append(entry['step'])\n",
                "        losses.append(entry['loss'])\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(steps, losses, label='Training Loss', color='blue')\n",
                "plt.xlabel('Steps')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Training Loss Curve')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "save_header"
            },
            "source": [
                "## 8. Save, Model Card & Push"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model_card"
            },
            "outputs": [],
            "source": [
                "# Create Model Card (README.md)\n",
                "model_card_content = f\"\"\"\n",
                "---\n",
                "base_model: {CONFIG['model_id']}\n",
                "library_name: peft\n",
                "license: mit\n",
                "tags:\n",
                "- finance\n",
                "- llama-3\n",
                "- 4-bit\n",
                "- loRA\n",
                "---\n",
                "\n",
                "# {CONFIG['new_model_name']}\n",
                "\n",
                "This model is a fine-tuned version of **{CONFIG['model_id']}** on the **Uber 2024 Annual Report** dataset. \n",
                "It is designed to act as a financial analyst \"Intern\", capable of answering questions with the specific strategy and tone of the source document.\n",
                "\n",
                "## Model Details\n",
                "- **Base Model:** {CONFIG['model_id']}\n",
                "- **Architecture:** 4-bit Quantization (NF4) + LoRA Adapters\n",
                "- **Task:** Financial Question Answering\n",
                "\n",
                "## Training Configuration\n",
                "- **LoRA Rank:** {CONFIG['lora_r']}\n",
                "- **LoRA Alpha:** {CONFIG['lora_alpha']}\n",
                "- **Batch Size:** {CONFIG['batch_size']}\n",
                "- **Learning Rate:** {CONFIG['learning_rate']}\n",
                "- **Epochs:** {CONFIG['num_epochs']}\n",
                "\n",
                "## Usage\n",
                "```python\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from peft import PeftModel, PeftConfig\n",
                "\n",
                "base_model_id = \"{CONFIG['model_id']}\"\n",
                "adapter_model_id = \"{CONFIG['hf_username']}/{CONFIG['new_model_name']}\"\n",
                "\n",
                "# Load Base Model\n",
                "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
                "# Load Adapter\n",
                "model = PeftModel.from_pretrained(base_model, adapter_model_id)\n",
                "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
                "\n",
                "input_text = \"What is the company's strategy for 2024?\"\n",
                "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
                "outputs = model.generate(**inputs, max_new_tokens=100)\n",
                "print(tokenizer.decode(outputs[0]))\n",
                "```\n",
                "\"\"\"\n",
                "\n",
                "# Ensure directory exists\n",
                "os.makedirs(CONFIG['new_model_name'], exist_ok=True)\n",
                "\n",
                "with open(f\"{CONFIG['new_model_name']}/README.md\", \"w\") as f:\n",
                "    f.write(model_card_content)\n",
                "\n",
                "print(f\"Model Card saved to {CONFIG['new_model_name']}/README.md\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_model"
            },
            "outputs": [],
            "source": [
                "new_model = CONFIG[\"new_model_name\"]\n",
                "trainer.model.save_pretrained(new_model)\n",
                "tokenizer.save_pretrained(new_model)\n",
                "# trainer.model.push_to_hub(new_model, use_temp_dir=False)\n",
                "# tokenizer.push_to_hub(new_model, use_temp_dir=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eval_header"
            },
            "source": [
                "## 9. Extended Evaluation (Golden Test Set)\n",
                "Evaluating on the `golden_test_set.jsonl` and saving detailed results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "inference_func"
            },
            "outputs": [],
            "source": [
                "def query_intern(question, model_to_use, tokenizer_to_use):\n",
                "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "\n",
                "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on context.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "\n",
                "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
                "    \n",
                "    inputs = tokenizer_to_use(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model_to_use.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=128,\n",
                "            temperature=0.1,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer_to_use.eos_token_id\n",
                "        )\n",
                "    \n",
                "    response = tokenizer_to_use.decode(outputs[0], skip_special_tokens=True)\n",
                "    return response.split(\"assistant\")[-1].strip()\n",
                "\n",
                "# Load Golden Test Set\n",
                "test_dataset = load_dataset(\"json\", data_files=\"../data/golden_test_set.jsonl\", split=\"train\")\n",
                "print(f\"Loaded {len(test_dataset)} test examples.\")\n",
                "\n",
                "# Evaluate on a subset (e.g., first 10 for demonstration, or all)\n",
                "results = []\n",
                "print(\"Running evaluation...\")\n",
                "\n",
                "# Load Base Model for comparison\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    CONFIG[\"model_id\"],\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    use_cache=True\n",
                ")\n",
                "\n",
                "for i, sample in enumerate(test_dataset):\n",
                "    if i >= 10: break # Limit to 10 for speed in notebook demo, remove for full run\n",
                "    \n",
                "    question = sample['question']\n",
                "    ground_truth = sample['answer']\n",
                "    \n",
                "    # Base Model Prediction\n",
                "    base_pred = query_intern(question, base_model, tokenizer)\n",
                "    \n",
                "    # Fine-Tuned Prediction\n",
                "    tuned_pred = query_intern(question, trainer.model, tokenizer)\n",
                "    \n",
                "    results.append({\n",
                "        \"question\": question,\n",
                "        \"ground_truth\": ground_truth,\n",
                "        \"base_model\": base_pred,\n",
                "        \"fine_tuned\": tuned_pred\n",
                "    })\n",
                "\n",
                "# Print Comparison\n",
                "import pandas as pd\n",
                "df_results = pd.DataFrame(results)\n",
                "pd.set_option('display.max_colwidth', None)\n",
                "\n",
                "print(\"\\n--- Visual Comparison (Top 5) ---\")\n",
                "display(df_results.head(5)) # Use display() in Jupyter, or print() otherwise\n",
                "\n",
                "# Save Results\n",
                "os.makedirs(\"../results\", exist_ok=True)\n",
                "df_results.to_json(\"../results/finetuned_results.jsonl\", orient=\"records\", lines=True)\n",
                "print(\"\\nResults saved to ../results/finetuned_results.jsonl\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
