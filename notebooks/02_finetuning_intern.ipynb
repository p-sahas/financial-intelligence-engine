{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_cell"
   },
   "source": [
    "# Part 2: \"The Intern\" (Fine-Tuning) - Refined Edition\n",
    "\n",
    "**Objective:** Fine-tune a Llama-3-8B model on the financial dataset to learn the strategy and tone of the 2024 Annual Report.\n",
    "\n",
    "**Key Features:**\n",
    "- **Reproducibility:** Strict random seed setting.\n",
    "- **Visualization:** Training loss plots and Base vs. Tuned comparisons.\n",
    "- **Extended Evaluation:** Testing on `golden_test_set.jsonl` and saving results.\n",
    "- **Optimization:** 4-bit quantization and LoRA optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## 1. Setup & Dependencies\n",
    "\n",
    "**IMPORTANT:** If you see an error like `AttributeError: partially initialized module 'torchvision'` or `ModuleNotFoundError: PreTrainedModel`, you **MUST**:\n",
    "1. Run the cell below.\n",
    "2. **Restart the Runtime** (Runtime -> Restart session).\n",
    "3. Run the valid imports cell (skip the install cell if you just ran it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchvision 0.20.1+cu121\n",
      "Uninstalling torchvision-0.20.1+cu121:\n",
      "  Successfully uninstalled torchvision-0.20.1+cu121\n",
      "Found existing installation: torch 2.5.1+cu121\n",
      "Uninstalling torch-2.5.1+cu121:\n",
      "  Successfully uninstalled torch-2.5.1+cu121\n",
      "Found existing installation: torchaudio 2.5.1+cu121\n",
      "Uninstalling torchaudio-2.5.1+cu121:\n",
      "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "3062c3e8766f461fb92499f878cd61f3",
       "pip_warning": {
        "packages": [
         "torch",
         "torchgen",
         "torchvision"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation complete. PLEASE RESTART RUNTIME now if this is the first run.\n"
     ]
    }
   ],
   "source": [
    "# 1. Uninstall potentially conflicting versions first\n",
    "!pip uninstall -y torchvision torch torchaudio\n",
    "\n",
    "# 2. Install compatible Torch + Torchvision\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# 3. Install other dependencies (including python-dotenv for local env)\n",
    "!pip install -q -U transformers peft trl bitsandbytes accelerate datasets scipy matplotlib python-dotenv\n",
    "\n",
    "print(\"Installation complete. PLEASE RESTART RUNTIME now if this is the first run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "Device: Tesla T4\n",
      "VRAM: 14.56 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Environment Check\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seeds_header"
   },
   "source": [
    "## 2. strict Reproducibility (Seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "set_seeds"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seeds set to 42 for reproducibility\n",
      " Note: Full determinism on GPU is not guaranteed due to non-deterministic operations\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Set environment variable for Python hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Set seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # Note: These settings may impact performance\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\" Seeds set to {SEED} for reproducibility\")\n",
    "print(\" Note: Full determinism on GPU is not guaranteed due to non-deterministic operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "path_config_header"
   },
   "source": [
    "## 3. Path Configuration & Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "path_config_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Google Colab Environment Detected.\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      " Google Drive Mounted at: /content/drive/MyDrive/Financial_Intern_Project\n",
      " Please upload 'train.jsonl' to: /content/drive/MyDrive/Financial_Intern_Project/data/processed\n",
      "Results will be saved to: /content/drive/MyDrive/Financial_Intern_Project/results\n",
      " Data Path: /content/drive/MyDrive/Financial_Intern_Project/data/processed\n",
      " Output Path: /content/drive/MyDrive/Financial_Intern_Project/results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Define the User's Local Path\n",
    "USER_LOCAL_ROOT = r\"C:/Development/financial-intelligence-engine\"\n",
    "USER_LOCAL_DATA = os.path.join(USER_LOCAL_ROOT, \"data/processed\")\n",
    "USER_LOCAL_OUTPUT = os.path.join(USER_LOCAL_ROOT, \"data/results\")\n",
    "\n",
    "# 2. Check Environment\n",
    "if os.path.exists(USER_LOCAL_ROOT):\n",
    "    # Running Locally (Windows/VS Code Local Kernel)\n",
    "    print(\" Local Windows Environment Detected.\")\n",
    "    BASE_PATH = USER_LOCAL_ROOT\n",
    "    DATA_PATH = USER_LOCAL_DATA\n",
    "    OUTPUT_PATH = USER_LOCAL_OUTPUT\n",
    "    \n",
    "elif 'google.colab' in sys.modules:\n",
    "    # Running in Colab\n",
    "    print(\" Google Colab Environment Detected.\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Define Drive Structure\n",
    "    DRIVE_ROOT = \"/content/drive/MyDrive/Financial_Intern_Project\"\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_ROOT}/data\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_ROOT}/results\", exist_ok=True)\n",
    "    \n",
    "    BASE_PATH = DRIVE_ROOT\n",
    "    DATA_PATH = f\"{DRIVE_ROOT}/data/processed\"\n",
    "    OUTPUT_PATH = f\"{DRIVE_ROOT}/results\"\n",
    "    \n",
    "    print(f\" Google Drive Mounted at: {BASE_PATH}\")\n",
    "    print(f\" Please upload 'train.jsonl' to: {DATA_PATH}\")\n",
    "    print(f\"Results will be saved to: {OUTPUT_PATH}\")\n",
    "    \n",
    "elif os.path.exists(\"/mnt/c\"):\n",
    "    # WSL\n",
    "    print(\" WSL Detected.\")\n",
    "    BASE_PATH = \"/mnt/c/Development/financial-intelligence-engine\"\n",
    "    DATA_PATH = f\"{BASE_PATH}/data/processed\"\n",
    "    OUTPUT_PATH = f\"{BASE_PATH}/data/results\"\n",
    "else:\n",
    "    # Generic Local / Repo Root\n",
    "    print(\"Generic Environment. Using relative paths.\")\n",
    "    BASE_PATH = \".\"\n",
    "    DATA_PATH = \"../data/processed\"\n",
    "    OUTPUT_PATH = \"../data/results\"\n",
    "\n",
    "print(f\" Data Path: {DATA_PATH}\")\n",
    "print(f\" Output Path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_header"
   },
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "config_cell"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Model\n",
    "    # \"model_id\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"model_id\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"new_model_name\": \"Llama-3-8b-Financial-Intern\",\n",
    "    \"hf_username\": \"p-sahas\", \n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_r\": 64,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 1,\n",
    "    \"grad_accumulation\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"max_steps\": 150,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 50,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"output_dir\": OUTPUT_PATH, # Uses dynamic path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auth_header"
   },
   "source": [
    "## 5. Hugging Face Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hf_auth"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab secret fetch skipped: Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.\n",
      "Automatic login failed: HF_TOKEN not found in environment variable or Colab secrets.\n",
      "Falling back to manual login...\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Attempt to load .env file if available (local usage)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, skipping .env loading (pip install python-dotenv)\")\n",
    "\n",
    "try:\n",
    "    # 1. Try Environment Variable (from .env or system)\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    \n",
    "    # 2. If missing, Try Colab Secrets\n",
    "    if not token:\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            token = userdata.get('HF_TOKEN')\n",
    "        except ImportError:\n",
    "            pass # Not in Colab\n",
    "        except Exception as e:\n",
    "            print(f\"Colab secret fetch skipped: {e}\")\n",
    "\n",
    "    # 3. Login or Fallback\n",
    "    if token:\n",
    "        print(f\"Logging in with token: {token[:4]}...{token[-4:]}\")\n",
    "        login(token)\n",
    "    else:\n",
    "        raise ValueError(\"HF_TOKEN not found in environment variable or Colab secrets.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Fallback to interactive login\n",
    "    print(f\"Automatic login failed: {e}\")\n",
    "    print(\"Falling back to manual login...\")\n",
    "    login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_header"
   },
   "source": [
    "## 6. Data Loading & Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "data_load"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset not found in known locations. Searched: ['/content/drive/MyDrive/Financial_Intern_Project/data/processed/train.jsonl', 'train.jsonl', '../data/processed/train.jsonl', '/content/train.jsonl']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "CRITICAL: Dataset 'train.jsonl' not found. Cannot proceed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-279421069.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" Dataset not found in known locations. Searched: {possible_paths}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CRITICAL: Dataset 'train.jsonl' not found. Cannot proceed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using dataset path: {dataset_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: CRITICAL: Dataset 'train.jsonl' not found. Cannot proceed."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Use the dynamic DATA_PATH configured earlier\n",
    "dataset_filename = \"train.jsonl\"\n",
    "possible_paths = [\n",
    "    os.path.join(DATA_PATH, dataset_filename),  # Path from Config (Local/Drive)\n",
    "    \"train.jsonl\",                              # Copy in CWD\n",
    "    \"../data/processed/train.jsonl\",                      # Relative fallback\n",
    "    \"/content/train.jsonl\"                      # Colab root\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        dataset_path = path\n",
    "        break\n",
    "\n",
    "if not dataset_path:\n",
    "    print(f\" Dataset not found in known locations. Searched: {possible_paths}\")\n",
    "    raise FileNotFoundError(\"CRITICAL: Dataset 'train.jsonl' not found. Cannot proceed.\")\n",
    "\n",
    "print(f\"Using dataset path: {dataset_path}\")\n",
    "try:\n",
    "    train_dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "    print(f\" Loaded {len(train_dataset)} training examples.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load dataset: {e}\")\n",
    "\n",
    "def format_instruction(sample):\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on context.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{sample['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{sample['answer']}<|eot_id|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_header"
   },
   "source": [
    "## 7. Model Loading (4-Bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_load"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71558f03da924cd4b6237c9ecba6e750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 27.81 MiB is free. Including non-PyTorch memory, this process has 14.53 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 101.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-226046114.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    375\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4061\u001b[0m             \u001b[0mdownload_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4062\u001b[0m         )\n\u001b[0;32m-> 4063\u001b[0;31m         \u001b[0mloading_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisk_offload_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4064\u001b[0m         \u001b[0mloading_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize_model_loading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloading_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4065\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set model in evaluation mode to deactivate Dropout modules by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(model, state_dict, checkpoint_files, load_config)\u001b[0m\n\u001b[1;32m   4180\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Neither a state dict nor checkpoint files were found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4182\u001b[0;31m             loading_info, disk_offload_index = convert_and_load_state_dict_in_model(\n\u001b[0m\u001b[1;32m   4183\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_state_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36mconvert_and_load_state_dict_in_model\u001b[0;34m(model, state_dict, load_config, tp_plan, disk_offload_index)\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                     realized_value = mapping.convert(\n\u001b[0m\u001b[1;32m   1193\u001b[0m                         \u001b[0mfirst_param_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, layer_name, model, config, hf_quantizer, loading_info)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;31m# Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# attribute during the whole process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mcollected_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaterialize_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Perform renaming op (for a simple WeightRenaming, `self.source_patterns` and `self.target_patterns` can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36mmaterialize_tensors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0;31m# Async loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m             \u001b[0;31m# Sync loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36m_job\u001b[0;34m()\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_materialize_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mthread_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/core_model_loading.py\u001b[0m in \u001b[0;36m_materialize_copy\u001b[0;34m(tensor, device, dtype)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 27.81 MiB is free. Including non-PyTorch memory, this process has 14.53 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 101.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# 1. Clear Cache before loading\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")\n",
    "\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading model (4-bit)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_id\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",         # <--- Changed to auto for better dispatch\n",
    "    offload_buffers=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_cache=False,\n",
    ")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_id\"], trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_header"
   },
   "source": [
    "## 8. Training & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3747996446.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m trainer = SFTTrainer(\n\u001b[1;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mpeft_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mformatting_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_instruction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=CONFIG[\"target_modules\"]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"grad_accumulation\"],\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=CONFIG[\"max_steps\"],\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=format_instruction,\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loss_plot"
   },
   "outputs": [],
   "source": [
    "# Visualize Training Loss\n",
    "history = trainer.state.log_history\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "for entry in history:\n",
    "    if 'loss' in entry:\n",
    "        steps.append(entry['step'])\n",
    "        losses.append(entry['loss'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_header"
   },
   "source": [
    "## 9. Save, Model Card & Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_card"
   },
   "outputs": [],
   "source": [
    "# Create Model Card (README.md)\n",
    "model_card_content = f\"\"\"\n",
    "---\n",
    "base_model: {CONFIG['model_id']}\n",
    "library_name: peft\n",
    "license: mit\n",
    "tags:\n",
    "- finance\n",
    "- llama-3\n",
    "- 4-bit\n",
    "- loRA\n",
    "---\n",
    "\n",
    "# {CONFIG['new_model_name']}\n",
    "\n",
    "This model is a fine-tuned version of **{CONFIG['model_id']}** on the **Uber 2024 Annual Report** dataset. \n",
    "It is designed to act as a financial analyst \"Intern\", capable of answering questions with the specific strategy and tone of the source document.\n",
    "\n",
    "## Model Details\n",
    "- **Base Model:** {CONFIG['model_id']}\n",
    "- **Architecture:** 4-bit Quantization (NF4) + LoRA Adapters\n",
    "- **Task:** Financial Question Answering\n",
    "\n",
    "## Training Configuration\n",
    "- **LoRA Rank:** {CONFIG['lora_r']}\n",
    "- **LoRA Alpha:** {CONFIG['lora_alpha']}\n",
    "- **Batch Size:** {CONFIG['batch_size']}\n",
    "- **Learning Rate:** {CONFIG['learning_rate']}\n",
    "- **Epochs:** {CONFIG['num_epochs']}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "base_model_id = \"{CONFIG['model_id']}\"\n",
    "adapter_model_id = \"{CONFIG['hf_username']}/{CONFIG['new_model_name']}\"\n",
    "\n",
    "# Load Base Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "# Load Adapter\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "input_text = \"What is the company's strategy for 2024?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(CONFIG['new_model_name'], exist_ok=True)\n",
    "\n",
    "with open(f\"{CONFIG['new_model_name']}/README.md\", \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "\n",
    "print(f\"Model Card saved to {CONFIG['new_model_name']}/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "new_model = CONFIG[\"new_model_name\"]\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "# trainer.model.push_to_hub(new_model, use_temp_dir=False)\n",
    "# tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval_header"
   },
   "source": [
    "## 10. Extended Evaluation (Golden Test Set)\n",
    "Evaluating on the `golden_test_set.jsonl` and saving detailed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_func"
   },
   "outputs": [],
   "source": [
    "def query_intern(question, model_to_use, tokenizer_to_use):\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on context.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    \n",
    "    inputs = tokenizer_to_use(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_to_use.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_to_use.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer_to_use.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"assistant\")[-1].strip()\n",
    "\n",
    "# Load Golden Test Set\n",
    "test_dataset = load_dataset(\"json\", data_files=\"../data/processed/golden_test_set.jsonl\", split=\"train\")\n",
    "print(f\"Loaded {len(test_dataset)} test examples.\")\n",
    "\n",
    "# Evaluate on a subset (e.g., first 10 for demonstration, or all)\n",
    "results = []\n",
    "print(\"Running evaluation...\")\n",
    "\n",
    "# Load Base Model for comparison\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_id\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    if i >= 10: break # Limit to 10 for speed in notebook demo, remove for full run\n",
    "    \n",
    "    question = sample['question']\n",
    "    ground_truth = sample['answer']\n",
    "    \n",
    "    # Base Model Prediction\n",
    "    base_pred = query_intern(question, base_model, tokenizer)\n",
    "    \n",
    "    # Fine-Tuned Prediction\n",
    "    tuned_pred = query_intern(question, trainer.model, tokenizer)\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"base_model\": base_pred,\n",
    "        \"fine_tuned\": tuned_pred\n",
    "    })\n",
    "\n",
    "# Print Comparison\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"\\n--- Visual Comparison (Top 5) ---\")\n",
    "display(df_results.head(5)) # Use display() in Jupyter, or print() otherwise\n",
    "\n",
    "# Save Results\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "df_results.to_json(\"../results/finetuned_results.jsonl\", orient=\"records\", lines=True)\n",
    "print(\"\\nResults saved to ../results/finetuned_results.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
