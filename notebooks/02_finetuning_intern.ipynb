{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_cell"
            },
            "source": [
                "# Part 2: \"The Intern\" (Fine-Tuning) - Refined Edition\n",
                "\n",
                "**Objective:** Fine-tune a Llama-3-8B model on the financial dataset to learn the strategy and tone of the 2024 Annual Report.\n",
                "\n",
                "**Key Features:**\n",
                "- **Reproducibility:** Strict random seed setting.\n",
                "- **Visualization:** Training loss plots and Base vs. Tuned comparisons.\n",
                "- **Extended Evaluation:** Testing on `golden_test_set.jsonl` and saving results.\n",
                "- **Optimization:** 4-bit quantization and LoRA optimization."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup_header"
            },
            "source": [
                "## 1. Setup & Dependencies\n",
                "\n",
                "**IMPORTANT:** If you see an error like `AttributeError: partially initialized module 'torchvision'` or `ModuleNotFoundError: PreTrainedModel`, you **MUST**:\n",
                "1. Run the cell below.\n",
                "2. **Restart the Runtime** (Runtime -> Restart session).\n",
                "3. Run the valid imports cell (skip the install cell if you just ran it)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found existing installation: torchvision 0.20.1+cu121\n",
                        "Uninstalling torchvision-0.20.1+cu121:\n",
                        "  Successfully uninstalled torchvision-0.20.1+cu121\n",
                        "Found existing installation: torch 2.5.1+cu121\n",
                        "Uninstalling torch-2.5.1+cu121:\n",
                        "  Successfully uninstalled torch-2.5.1+cu121\n",
                        "Found existing installation: torchaudio 2.5.1+cu121\n",
                        "Uninstalling torchaudio-2.5.1+cu121:\n",
                        "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
                        "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
                        "Collecting torch\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
                        "Collecting torchvision\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
                        "Collecting torchaudio\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
                        "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
                        "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
                        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch) (9.1.0.70)\n",
                        "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.3.1)\n",
                        "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch) (11.0.2.54)\n",
                        "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.2.106)\n",
                        "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch) (11.4.5.107)\n",
                        "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.0.106)\n",
                        "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.21.5)\n",
                        "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.0)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
                        "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
                        "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
                        "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
                        "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
                        "Installing collected packages: torch, torchvision, torchaudio\n",
                        "Successfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n",
                        "Installation complete. PLEASE RESTART RUNTIME now if this is the first run.\n"
                    ]
                }
            ],
            "source": [
                "# 1. Uninstall potentially conflicting versions first\n",
                "!pip uninstall -y torchvision torch torchaudio\n",
                "\n",
                "# 2. Install compatible Torch + Torchvision\n",
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
                "\n",
                "# 3. Install other dependencies (including python-dotenv for local env)\n",
                "!pip install -q -U transformers peft trl bitsandbytes accelerate datasets scipy matplotlib python-dotenv\n",
                "\n",
                "print(\"Installation complete. PLEASE RESTART RUNTIME now if this is the first run.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "imports_and_env"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch Version: 2.5.1+cu121\n",
                        "CUDA Available: True\n",
                        "Device: Tesla T4\n",
                        "VRAM: 14.56 GB\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import os\n",
                "import random\n",
                "import numpy as np\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                "    logging,\n",
                ")\n",
                "from peft import LoraConfig, PeftModel\n",
                "from trl import SFTTrainer\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Environment Check\n",
                "print(f\"PyTorch Version: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "seeds_header"
            },
            "source": [
                "## 2. strict Reproducibility (Seeds)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "set_seeds"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " Seeds set to 42 for reproducibility\n",
                        " Note: Full determinism on GPU is not guaranteed due to non-deterministic operations\n"
                    ]
                }
            ],
            "source": [
                "SEED = 42\n",
                "\n",
                "# Set environment variable for Python hash seed\n",
                "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
                "\n",
                "# Set seeds\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed(SEED)\n",
                "    torch.cuda.manual_seed_all(SEED)\n",
                "    # Note: These settings may impact performance\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "\n",
                "print(f\" Seeds set to {SEED} for reproducibility\")\n",
                "print(\" Note: Full determinism on GPU is not guaranteed due to non-deterministic operations\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "path_config_header"
            },
            "source": [
                "## 3. Path Configuration & Storage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "path_config_cell"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " Google Colab Environment Detected.\n",
                        "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
                        " Google Drive Mounted at: /content/drive/MyDrive/Financial_Intern_Project\n",
                        " Please upload 'train.jsonl' to: /content/drive/MyDrive/Financial_Intern_Project/data/processed\n",
                        "Results will be saved to: /content/drive/MyDrive/Financial_Intern_Project/results\n",
                        " Data Path: /content/drive/MyDrive/Financial_Intern_Project/data/processed\n",
                        " Output Path: /content/drive/MyDrive/Financial_Intern_Project/results\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# 1. Define the User's Local Path\n",
                "USER_LOCAL_ROOT = r\"C:/Development/financial-intelligence-engine\"\n",
                "USER_LOCAL_DATA = os.path.join(USER_LOCAL_ROOT, \"data/processed\")\n",
                "USER_LOCAL_OUTPUT = os.path.join(USER_LOCAL_ROOT, \"data/results\")\n",
                "\n",
                "# 2. Check Environment\n",
                "if os.path.exists(USER_LOCAL_ROOT):\n",
                "    # Running Locally (Windows/VS Code Local Kernel)\n",
                "    print(\" Local Windows Environment Detected.\")\n",
                "    BASE_PATH = USER_LOCAL_ROOT\n",
                "    DATA_PATH = USER_LOCAL_DATA\n",
                "    OUTPUT_PATH = USER_LOCAL_OUTPUT\n",
                "    \n",
                "elif 'google.colab' in sys.modules:\n",
                "    # Running in Colab\n",
                "    print(\" Google Colab Environment Detected.\")\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    \n",
                "    # Define Drive Structure\n",
                "    DRIVE_ROOT = \"/content/drive/MyDrive/Financial_Intern_Project\"\n",
                "    \n",
                "    # Create directories if they don't exist\n",
                "    os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
                "    os.makedirs(f\"{DRIVE_ROOT}/data\", exist_ok=True)\n",
                "    os.makedirs(f\"{DRIVE_ROOT}/results\", exist_ok=True)\n",
                "    \n",
                "    BASE_PATH = DRIVE_ROOT\n",
                "    DATA_PATH = f\"{DRIVE_ROOT}/data/processed\"\n",
                "    OUTPUT_PATH = f\"{DRIVE_ROOT}/results\"\n",
                "    \n",
                "    print(f\" Google Drive Mounted at: {BASE_PATH}\")\n",
                "    print(f\" Please upload 'train.jsonl' to: {DATA_PATH}\")\n",
                "    print(f\"Results will be saved to: {OUTPUT_PATH}\")\n",
                "    \n",
                "elif os.path.exists(\"/mnt/c\"):\n",
                "    # WSL\n",
                "    print(\" WSL Detected.\")\n",
                "    BASE_PATH = \"/mnt/c/Development/financial-intelligence-engine\"\n",
                "    DATA_PATH = f\"{BASE_PATH}/data/processed\"\n",
                "    OUTPUT_PATH = f\"{BASE_PATH}/data/results\"\n",
                "else:\n",
                "    # Generic Local / Repo Root\n",
                "    print(\"Generic Environment. Using relative paths.\")\n",
                "    BASE_PATH = \".\"\n",
                "    DATA_PATH = \"../data/processed\"\n",
                "    OUTPUT_PATH = \"../data/results\"\n",
                "\n",
                "print(f\" Data Path: {DATA_PATH}\")\n",
                "print(f\" Output Path: {OUTPUT_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "config_header"
            },
            "source": [
                "## 4. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "id": "config_cell"
            },
            "outputs": [],
            "source": [
                "CONFIG = {\n",
                "    # Model\n",
                "    # \"model_id\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
                "    \"model_id\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
                "    \"new_model_name\": \"Llama-3-8b-Financial-Intern\",\n",
                "    \"hf_username\": \"p-sahas\", \n",
                "    \n",
                "    # LoRA\n",
                "    \"lora_r\": 64,\n",
                "    \"lora_alpha\": 16,\n",
                "    \"lora_dropout\": 0.1,\n",
                "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    \n",
                "    # Training\n",
                "    \"batch_size\": 1,\n",
                "    \"grad_accumulation\": 4,\n",
                "    \"learning_rate\": 2e-4,\n",
                "    \"num_epochs\": 1,\n",
                "    \"max_steps\": 150,\n",
                "    \"logging_steps\": 10,\n",
                "    \"save_steps\": 50,\n",
                "    \"max_seq_length\": 512,\n",
                "    \"output_dir\": OUTPUT_PATH, # Uses dynamic path\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "auth_header"
            },
            "source": [
                "## 5. Hugging Face Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "id": "hf_auth"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Colab secret fetch skipped: Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.\n",
                        "Automatic login failed: HF_TOKEN not found in environment variable or Colab secrets.\n",
                        "Falling back to manual login...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
                        "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
                        "You are not authenticated with the Hugging Face Hub in this notebook.\n",
                        "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "from huggingface_hub import login\n",
                "import os\n",
                "\n",
                "# Attempt to load .env file if available (local usage)\n",
                "try:\n",
                "    from dotenv import load_dotenv\n",
                "    load_dotenv()\n",
                "except ImportError:\n",
                "    print(\"python-dotenv not installed, skipping .env loading (pip install python-dotenv)\")\n",
                "\n",
                "try:\n",
                "    # 1. Try Environment Variable (from .env or system)\n",
                "    token = os.getenv(\"HF_TOKEN\")\n",
                "    \n",
                "    # 2. If missing, Try Colab Secrets\n",
                "    if not token:\n",
                "        try:\n",
                "            from google.colab import userdata\n",
                "            token = userdata.get('HF_TOKEN')\n",
                "        except ImportError:\n",
                "            pass # Not in Colab\n",
                "        except Exception as e:\n",
                "            print(f\"Colab secret fetch skipped: {e}\")\n",
                "\n",
                "    # 3. Login or Fallback\n",
                "    if token:\n",
                "        print(f\"Logging in with token: {token[:4]}...{token[-4:]}\")\n",
                "        login(token)\n",
                "    else:\n",
                "        raise ValueError(\"HF_TOKEN not found in environment variable or Colab secrets.\")\n",
                "\n",
                "except Exception as e:\n",
                "    # Fallback to interactive login\n",
                "    print(f\"Automatic login failed: {e}\")\n",
                "    print(\"Falling back to manual login...\")\n",
                "    login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "data_header"
            },
            "source": [
                "## 6. Data Loading & Formatting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "data_load"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " Dataset not found in known locations. Searched: ['/content/drive/MyDrive/Financial_Intern_Project/data/processed/train.jsonl', 'train.jsonl', '../data/processed/train.jsonl', '/content/train.jsonl']\n",
                        " If in Colab: Please upload 'train.jsonl' to your Drive (Financia_Intern_Project/data).\n",
                        " If Local: Check that 'C:/Development/financial-intelligence-engine/data/train.jsonl' exists.\n",
                        "Using dataset path: train.jsonl\n",
                        "CRITICAL: Dataset file definitely missing. Please check path.\n"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import os\n",
                "\n",
                "# Use the dynamic DATA_PATH configured earlier\n",
                "dataset_filename = \"train.jsonl\"\n",
                "possible_paths = [\n",
                "    os.path.join(DATA_PATH, dataset_filename),  # Path from Config (Local/Drive)\n",
                "    \"train.jsonl\",                              # Copy in CWD\n",
                "    \"../data/processed/train.jsonl\",                      # Relative fallback\n",
                "    \"/content/train.jsonl\"                      # Colab root\n",
                "]\n",
                "\n",
                "dataset_path = None\n",
                "for path in possible_paths:\n",
                "    if os.path.exists(path):\n",
                "        dataset_path = path\n",
                "        break\n",
                "\n",
                "if not dataset_path:\n",
                "    print(f\" Dataset not found in known locations. Searched: {possible_paths}\")\n",
                "    print(\" If in Colab: Please upload 'train.jsonl' to your Drive (Financia_Intern_Project/data).\")\n",
                "    print(\" If Local: Check that 'C:/Development/financial-intelligence-engine/data/train.jsonl' exists.\")\n",
                "    \n",
                "    # Last resort fallback\n",
                "    dataset_path = \"train.jsonl\" \n",
                "\n",
                "print(f\"Using dataset path: {dataset_path}\")\n",
                "try:\n",
                "    train_dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
                "    print(f\" Loaded {len(train_dataset)} training examples.\")\n",
                "except FileNotFoundError:\n",
                "    print(\"CRITICAL: Dataset file definitely missing. Please check path.\")\n",
                "\n",
                "def format_instruction(sample):\n",
                "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "\n",
                "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on context.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "\n",
                "{sample['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
                "\n",
                "{sample['answer']}<|eot_id|>\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "model_header"
            },
            "source": [
                "## 7. Model Loading (4-Bit)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "id": "model_load"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Current model requires 512 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0cc0430d12b44536807d69bf2d728769",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mCanceled future for execute_request message before replies were done"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    CONFIG[\"model_id\"],\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    offload_buffers=True,      # <--- Add this to resolve the 512-byte buffer error\n",
                "    low_cpu_mem_usage=True,    # <--- Helps manage RAM during the load process\n",
                "    use_cache=False,\n",
                ")\n",
                "model.config.pretraining_tp = 1\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_id\"], trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "train_header"
            },
            "source": [
                "## 8. Training & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'train_dataset' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-3327077036.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m trainer = SFTTrainer(\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mpeft_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mformatting_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_instruction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
                    ]
                }
            ],
            "source": [
                "peft_config = LoraConfig(\n",
                "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
                "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
                "    r=CONFIG[\"lora_r\"],\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    target_modules=CONFIG[\"target_modules\"]\n",
                ")\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=CONFIG[\"output_dir\"],\n",
                "    gradient_checkpointing=True,\n",
                "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
                "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
                "    gradient_accumulation_steps=CONFIG[\"grad_accumulation\"],\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    save_steps=CONFIG[\"save_steps\"],\n",
                "    logging_steps=CONFIG[\"logging_steps\"],\n",
                "    learning_rate=CONFIG[\"learning_rate\"],\n",
                "    weight_decay=0.001,\n",
                "    fp16=True,\n",
                "    bf16=False,\n",
                "    max_grad_norm=0.3,\n",
                "    max_steps=CONFIG[\"max_steps\"],\n",
                "    warmup_ratio=0.03,\n",
                "    group_by_length=True,\n",
                "    lr_scheduler_type=\"constant\",\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=train_dataset,\n",
                "    peft_config=peft_config,\n",
                "    formatting_func=format_instruction,\n",
                "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "print(\"Starting training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "loss_plot"
            },
            "outputs": [],
            "source": [
                "# Visualize Training Loss\n",
                "history = trainer.state.log_history\n",
                "steps = []\n",
                "losses = []\n",
                "\n",
                "for entry in history:\n",
                "    if 'loss' in entry:\n",
                "        steps.append(entry['step'])\n",
                "        losses.append(entry['loss'])\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(steps, losses, label='Training Loss', color='blue')\n",
                "plt.xlabel('Steps')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Training Loss Curve')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "save_header"
            },
            "source": [
                "## 9. Save, Model Card & Push"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model_card"
            },
            "outputs": [],
            "source": [
                "# Create Model Card (README.md)\n",
                "model_card_content = f\"\"\"\n",
                "---\n",
                "base_model: {CONFIG['model_id']}\n",
                "library_name: peft\n",
                "license: mit\n",
                "tags:\n",
                "- finance\n",
                "- llama-3\n",
                "- 4-bit\n",
                "- loRA\n",
                "---\n",
                "\n",
                "# {CONFIG['new_model_name']}\n",
                "\n",
                "This model is a fine-tuned version of **{CONFIG['model_id']}** on the **Uber 2024 Annual Report** dataset. \n",
                "It is designed to act as a financial analyst \"Intern\", capable of answering questions with the specific strategy and tone of the source document.\n",
                "\n",
                "## Model Details\n",
                "- **Base Model:** {CONFIG['model_id']}\n",
                "- **Architecture:** 4-bit Quantization (NF4) + LoRA Adapters\n",
                "- **Task:** Financial Question Answering\n",
                "\n",
                "## Training Configuration\n",
                "- **LoRA Rank:** {CONFIG['lora_r']}\n",
                "- **LoRA Alpha:** {CONFIG['lora_alpha']}\n",
                "- **Batch Size:** {CONFIG['batch_size']}\n",
                "- **Learning Rate:** {CONFIG['learning_rate']}\n",
                "- **Epochs:** {CONFIG['num_epochs']}\n",
                "\n",
                "## Usage\n",
                "```python\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from peft import PeftModel, PeftConfig\n",
                "\n",
                "base_model_id = \"{CONFIG['model_id']}\"\n",
                "adapter_model_id = \"{CONFIG['hf_username']}/{CONFIG['new_model_name']}\"\n",
                "\n",
                "# Load Base Model\n",
                "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
                "# Load Adapter\n",
                "model = PeftModel.from_pretrained(base_model, adapter_model_id)\n",
                "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
                "\n",
                "input_text = \"What is the company's strategy for 2024?\"\n",
                "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
                "outputs = model.generate(**inputs, max_new_tokens=100)\n",
                "print(tokenizer.decode(outputs[0]))\n",
                "```\n",
                "\"\"\"\n",
                "\n",
                "# Ensure directory exists\n",
                "os.makedirs(CONFIG['new_model_name'], exist_ok=True)\n",
                "\n",
                "with open(f\"{CONFIG['new_model_name']}/README.md\", \"w\") as f:\n",
                "    f.write(model_card_content)\n",
                "\n",
                "print(f\"Model Card saved to {CONFIG['new_model_name']}/README.md\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_model"
            },
            "outputs": [],
            "source": [
                "new_model = CONFIG[\"new_model_name\"]\n",
                "trainer.model.save_pretrained(new_model)\n",
                "tokenizer.save_pretrained(new_model)\n",
                "# trainer.model.push_to_hub(new_model, use_temp_dir=False)\n",
                "# tokenizer.push_to_hub(new_model, use_temp_dir=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eval_header"
            },
            "source": [
                "## 10. Extended Evaluation (Golden Test Set)\n",
                "Evaluating on the `golden_test_set.jsonl` and saving detailed results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "inference_func"
            },
            "outputs": [],
            "source": [
                "def query_intern(question, model_to_use, tokenizer_to_use):\n",
                "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "\n",
                "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on context.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "\n",
                "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
                "    \n",
                "    inputs = tokenizer_to_use(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model_to_use.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=128,\n",
                "            temperature=0.1,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer_to_use.eos_token_id\n",
                "        )\n",
                "    \n",
                "    response = tokenizer_to_use.decode(outputs[0], skip_special_tokens=True)\n",
                "    return response.split(\"assistant\")[-1].strip()\n",
                "\n",
                "# Load Golden Test Set\n",
                "test_dataset = load_dataset(\"json\", data_files=\"../data/processed/golden_test_set.jsonl\", split=\"train\")\n",
                "print(f\"Loaded {len(test_dataset)} test examples.\")\n",
                "\n",
                "# Evaluate on a subset (e.g., first 10 for demonstration, or all)\n",
                "results = []\n",
                "print(\"Running evaluation...\")\n",
                "\n",
                "# Load Base Model for comparison\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    CONFIG[\"model_id\"],\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    use_cache=True\n",
                ")\n",
                "\n",
                "for i, sample in enumerate(test_dataset):\n",
                "    if i >= 10: break # Limit to 10 for speed in notebook demo, remove for full run\n",
                "    \n",
                "    question = sample['question']\n",
                "    ground_truth = sample['answer']\n",
                "    \n",
                "    # Base Model Prediction\n",
                "    base_pred = query_intern(question, base_model, tokenizer)\n",
                "    \n",
                "    # Fine-Tuned Prediction\n",
                "    tuned_pred = query_intern(question, trainer.model, tokenizer)\n",
                "    \n",
                "    results.append({\n",
                "        \"question\": question,\n",
                "        \"ground_truth\": ground_truth,\n",
                "        \"base_model\": base_pred,\n",
                "        \"fine_tuned\": tuned_pred\n",
                "    })\n",
                "\n",
                "# Print Comparison\n",
                "import pandas as pd\n",
                "df_results = pd.DataFrame(results)\n",
                "pd.set_option('display.max_colwidth', None)\n",
                "\n",
                "print(\"\\n--- Visual Comparison (Top 5) ---\")\n",
                "display(df_results.head(5)) # Use display() in Jupyter, or print() otherwise\n",
                "\n",
                "# Save Results\n",
                "os.makedirs(\"../results\", exist_ok=True)\n",
                "df_results.to_json(\"../results/finetuned_results.jsonl\", orient=\"records\", lines=True)\n",
                "print(\"\\nResults saved to ../results/finetuned_results.jsonl\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}