{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_cell"
            },
            "source": [
                "# Part 2: \"The Intern\" (Fine-Tuning) - Refined Edition\n",
                "\n",
                "**Objective:** Fine-tune a Llama-3-8B model on the financial dataset to learn the strategy and tone of the 2024 Annual Report.\n",
                "\n",
                "**Key Features:**\n",
                "- **Reproducibility:** Strict random seed setting.\n",
                "- **Visualization:** Training loss plots and Base vs. Tuned comparisons.\n",
                "- **Extended Evaluation:** Testing on `golden_test_set.jsonl` and saving results.\n",
                "- **Optimization:** 4-bit quantization and LoRA optimization."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup_header"
            },
            "source": [
                "## 1. Setup & Dependencies\n",
                "\n",
                "**IMPORTANT:** If you see an error like `AttributeError: partially initialized module 'torchvision'` or `ModuleNotFoundError: PreTrainedModel`, you **MUST**:\n",
                "1. Run the cell below.\n",
                "2. **Restart the Runtime** (Runtime -> Restart session).\n",
                "3. Run the valid imports cell (skip the install cell if you just ran it)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found existing installation: torchvision 0.20.1+cu121\n",
                        "Uninstalling torchvision-0.20.1+cu121:\n",
                        "  Successfully uninstalled torchvision-0.20.1+cu121\n",
                        "Found existing installation: torch 2.5.1+cu121\n",
                        "Uninstalling torch-2.5.1+cu121:\n",
                        "  Successfully uninstalled torch-2.5.1+cu121\n",
                        "Found existing installation: torchaudio 2.5.1+cu121\n",
                        "Uninstalling torchaudio-2.5.1+cu121:\n",
                        "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
                        "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
                        "Collecting torch\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
                        "Collecting torchvision\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
                        "Collecting torchaudio\n",
                        "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
                        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
                        "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
                        "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
                        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
                        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch) (9.1.0.70)\n",
                        "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.3.1)\n",
                        "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch) (11.0.2.54)\n",
                        "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.2.106)\n",
                        "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch) (11.4.5.107)\n",
                        "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.0.106)\n",
                        "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.21.5)\n",
                        "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
                        "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.0)\n",
                        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
                        "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
                        "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
                        "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
                        "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
                        "Installing collected packages: torch, torchvision, torchaudio\n",
                        "Successfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n",
                        "Installation complete. PLEASE RESTART RUNTIME now if this is the first run.\n"
                    ]
                }
            ],
            "source": [
                "# 1. Uninstall potentially conflicting versions first\n",
                "!pip uninstall -y torchvision torch torchaudio\n",
                "\n",
                "# 2. Install compatible Torch + Torchvision\n",
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
                "\n",
                "# 3. Install other dependencies\n",
                "!pip install -q -U transformers peft trl bitsandbytes accelerate datasets scipy matplotlib\n",
                "\n",
                "print(\"Installation complete. PLEASE RESTART RUNTIME now if this is the first run.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "imports_and_env"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch Version: 2.5.1+cu121\n",
                        "CUDA Available: True\n",
                        "Device: Tesla T4\n",
                        "VRAM: 14.56 GB\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import os\n",
                "import random\n",
                "import numpy as np\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                "    logging,\n",
                ")\n",
                "from peft import LoraConfig, PeftModel\n",
                "from trl import SFTTrainer\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Environment Check\n",
                "print(f\"PyTorch Version: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "seeds_header"
            },
            "source": [
                "## 2. strict Reproducibility (Seeds)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "set_seeds"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ Seeds set to 42 for reproducibility\n",
                        "⚠️ Note: Full determinism on GPU is not guaranteed due to non-deterministic operations\n"
                    ]
                }
            ],
            "source": [
                "SEED = 42\n",
                "\n",
                "# Set environment variable for Python hash seed\n",
                "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
                "\n",
                "# Set seeds\n",
                "random.seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.manual_seed(SEED)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.manual_seed(SEED)\n",
                "    torch.cuda.manual_seed_all(SEED)\n",
                "    # Note: These settings may impact performance\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "    torch.backends.cudnn.benchmark = False\n",
                "\n",
                "print(f\" Seeds set to {SEED} for reproducibility\")\n",
                "print(\" Note: Full determinism on GPU is not guaranteed due to non-deterministic operations\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "config_header"
            },
            "source": [
                "## 3. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "config_cell"
            },
            "outputs": [],
            "source": [
                "CONFIG = {\n",
                "    # Model\n",
                "    \"model_id\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
                "    \"new_model_name\": \"Llama-3-8b-Financial-Intern\",\n",
                "    \"hf_username\": \"p-sahas\", \n",
                "    \n",
                "    # LoRA\n",
                "    \"lora_r\": 64,\n",
                "    \"lora_alpha\": 16,\n",
                "    \"lora_dropout\": 0.1,\n",
                "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    \n",
                "    # Training\n",
                "    \"batch_size\": 4,\n",
                "    \"grad_accumulation\": 1,\n",
                "    \"learning_rate\": 2e-4,\n",
                "    \"num_epochs\": 1,\n",
                "    \"max_steps\": 150,\n",
                "    \"logging_steps\": 10,\n",
                "    \"save_steps\": 50,\n",
                "    \"max_seq_length\": 512,\n",
                "    \"output_dir\": \"./results\",\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "auth_header"
            },
            "source": [
                "## 4. Hugging Face Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "hf_auth"
            },
            "outputs": [
                {
                    "ename": "TimeoutException",
                    "evalue": "Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-565404535.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HF_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mTimeoutException\u001b[0m: Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI."
                    ]
                }
            ],
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "try:\n",
                "    from google.colab import userdata\n",
                "    token = userdata.get('HF_TOKEN')\n",
                "    login(token)\n",
                "except ImportError:\n",
                "    login() # Will prompt for token"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "data_header"
            },
            "source": [
                "## 5. Data Loading & Formatting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "data_load"
            },
            "outputs": [],
            "source": [
                "dataset_name = \"../data/train.jsonl\"\n",
                "train_dataset = load_dataset(\"json\", data_files=dataset_name, split=\"train\")\n",
                "\n",
                "def format_instruction(sample):\n",
                "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "\n",
                "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on context.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "\n",
                "{sample['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
                "\n",
                "{sample['answer']}<|eot_id|>\"\"\"\n",
                "\n",
                "print(f\"Loaded {len(train_dataset)} training examples.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "model_header"
            },
            "source": [
                "## 6. Model Loading (4-Bit)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model_load"
            },
            "outputs": [],
            "source": [
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    CONFIG[\"model_id\"],\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    use_cache=False,\n",
                ")\n",
                "model.config.pretraining_tp = 1\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_id\"], trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "train_header"
            },
            "source": [
                "## 7. Training & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "training"
            },
            "outputs": [],
            "source": [
                "peft_config = LoraConfig(\n",
                "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
                "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
                "    r=CONFIG[\"lora_r\"],\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    target_modules=CONFIG[\"target_modules\"]\n",
                ")\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=CONFIG[\"output_dir\"],\n",
                "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
                "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
                "    gradient_accumulation_steps=CONFIG[\"grad_accumulation\"],\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    save_steps=CONFIG[\"save_steps\"],\n",
                "    logging_steps=CONFIG[\"logging_steps\"],\n",
                "    learning_rate=CONFIG[\"learning_rate\"],\n",
                "    weight_decay=0.001,\n",
                "    fp16=True,\n",
                "    bf16=False,\n",
                "    max_grad_norm=0.3,\n",
                "    max_steps=CONFIG[\"max_steps\"],\n",
                "    warmup_ratio=0.03,\n",
                "    group_by_length=True,\n",
                "    lr_scheduler_type=\"constant\",\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=train_dataset,\n",
                "    peft_config=peft_config,\n",
                "    formatting_func=format_instruction,\n",
                "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "print(\"Starting training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "loss_plot"
            },
            "outputs": [],
            "source": [
                "# Visualize Training Loss\n",
                "history = trainer.state.log_history\n",
                "steps = []\n",
                "losses = []\n",
                "\n",
                "for entry in history:\n",
                "    if 'loss' in entry:\n",
                "        steps.append(entry['step'])\n",
                "        losses.append(entry['loss'])\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(steps, losses, label='Training Loss', color='blue')\n",
                "plt.xlabel('Steps')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('Training Loss Curve')\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "save_header"
            },
            "source": [
                "## 8. Save, Model Card & Push"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "model_card"
            },
            "outputs": [],
            "source": [
                "# Create Model Card (README.md)\n",
                "model_card_content = f\"\"\"\n",
                "---\n",
                "base_model: {CONFIG['model_id']}\n",
                "library_name: peft\n",
                "license: mit\n",
                "tags:\n",
                "- finance\n",
                "- llama-3\n",
                "- 4-bit\n",
                "- loRA\n",
                "---\n",
                "\n",
                "# {CONFIG['new_model_name']}\n",
                "\n",
                "This model is a fine-tuned version of **{CONFIG['model_id']}** on the **Uber 2024 Annual Report** dataset. \n",
                "It is designed to act as a financial analyst \"Intern\", capable of answering questions with the specific strategy and tone of the source document.\n",
                "\n",
                "## Model Details\n",
                "- **Base Model:** {CONFIG['model_id']}\n",
                "- **Architecture:** 4-bit Quantization (NF4) + LoRA Adapters\n",
                "- **Task:** Financial Question Answering\n",
                "\n",
                "## Training Configuration\n",
                "- **LoRA Rank:** {CONFIG['lora_r']}\n",
                "- **LoRA Alpha:** {CONFIG['lora_alpha']}\n",
                "- **Batch Size:** {CONFIG['batch_size']}\n",
                "- **Learning Rate:** {CONFIG['learning_rate']}\n",
                "- **Epochs:** {CONFIG['num_epochs']}\n",
                "\n",
                "## Usage\n",
                "```python\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from peft import PeftModel, PeftConfig\n",
                "\n",
                "base_model_id = \"{CONFIG['model_id']}\"\n",
                "adapter_model_id = \"{CONFIG['hf_username']}/{CONFIG['new_model_name']}\"\n",
                "\n",
                "# Load Base Model\n",
                "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
                "# Load Adapter\n",
                "model = PeftModel.from_pretrained(base_model, adapter_model_id)\n",
                "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
                "\n",
                "input_text = \"What is the company's strategy for 2024?\"\n",
                "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
                "outputs = model.generate(**inputs, max_new_tokens=100)\n",
                "print(tokenizer.decode(outputs[0]))\n",
                "```\n",
                "\"\"\"\n",
                "\n",
                "# Ensure directory exists\n",
                "os.makedirs(CONFIG['new_model_name'], exist_ok=True)\n",
                "\n",
                "with open(f\"{CONFIG['new_model_name']}/README.md\", \"w\") as f:\n",
                "    f.write(model_card_content)\n",
                "\n",
                "print(f\"Model Card saved to {CONFIG['new_model_name']}/README.md\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_model"
            },
            "outputs": [],
            "source": [
                "new_model = CONFIG[\"new_model_name\"]\n",
                "trainer.model.save_pretrained(new_model)\n",
                "tokenizer.save_pretrained(new_model)\n",
                "# trainer.model.push_to_hub(new_model, use_temp_dir=False)\n",
                "# tokenizer.push_to_hub(new_model, use_temp_dir=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eval_header"
            },
            "source": [
                "## 9. Extended Evaluation (Golden Test Set)\n",
                "Evaluating on the `golden_test_set.jsonl` and saving detailed results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "inference_func"
            },
            "outputs": [],
            "source": [
                "def query_intern(question, model_to_use, tokenizer_to_use):\n",
                "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "\n",
                "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on context.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "\n",
                "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
                "    \n",
                "    inputs = tokenizer_to_use(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model_to_use.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=128,\n",
                "            temperature=0.1,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer_to_use.eos_token_id\n",
                "        )\n",
                "    \n",
                "    response = tokenizer_to_use.decode(outputs[0], skip_special_tokens=True)\n",
                "    return response.split(\"assistant\")[-1].strip()\n",
                "\n",
                "# Load Golden Test Set\n",
                "test_dataset = load_dataset(\"json\", data_files=\"../data/golden_test_set.jsonl\", split=\"train\")\n",
                "print(f\"Loaded {len(test_dataset)} test examples.\")\n",
                "\n",
                "# Evaluate on a subset (e.g., first 10 for demonstration, or all)\n",
                "results = []\n",
                "print(\"Running evaluation...\")\n",
                "\n",
                "# Load Base Model for comparison\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    CONFIG[\"model_id\"],\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    use_cache=True\n",
                ")\n",
                "\n",
                "for i, sample in enumerate(test_dataset):\n",
                "    if i >= 10: break # Limit to 10 for speed in notebook demo, remove for full run\n",
                "    \n",
                "    question = sample['question']\n",
                "    ground_truth = sample['answer']\n",
                "    \n",
                "    # Base Model Prediction\n",
                "    base_pred = query_intern(question, base_model, tokenizer)\n",
                "    \n",
                "    # Fine-Tuned Prediction\n",
                "    tuned_pred = query_intern(question, trainer.model, tokenizer)\n",
                "    \n",
                "    results.append({\n",
                "        \"question\": question,\n",
                "        \"ground_truth\": ground_truth,\n",
                "        \"base_model\": base_pred,\n",
                "        \"fine_tuned\": tuned_pred\n",
                "    })\n",
                "\n",
                "# Print Comparison\n",
                "import pandas as pd\n",
                "df_results = pd.DataFrame(results)\n",
                "pd.set_option('display.max_colwidth', None)\n",
                "\n",
                "print(\"\\n--- Visual Comparison (Top 5) ---\")\n",
                "display(df_results.head(5)) # Use display() in Jupyter, or print() otherwise\n",
                "\n",
                "# Save Results\n",
                "os.makedirs(\"../results\", exist_ok=True)\n",
                "df_results.to_json(\"../results/finetuned_results.jsonl\", orient=\"records\", lines=True)\n",
                "print(\"\\nResults saved to ../results/finetuned_results.jsonl\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
