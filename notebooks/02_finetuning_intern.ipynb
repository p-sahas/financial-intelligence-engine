{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_cell"
   },
   "source": [
    "# Part 2: \"The Intern\" (Fine-Tuning) - Refined Edition\n",
    "\n",
    "**Objective:** Fine-tune a Llama-3-8B model on the financial dataset to learn the strategy and tone of the 2024 Annual Report.\n",
    "\n",
    "**Key Features:**\n",
    "- **Reproducibility:** Strict random seed setting.\n",
    "- **Visualization:** Training loss plots and Base vs. Tuned comparisons.\n",
    "- **Extended Evaluation:** Testing on `golden_test_set.jsonl` and saving results.\n",
    "- **Optimization:** 4-bit quantization and LoRA optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## 1. Setup & Dependencies\n",
    "\n",
    "**IMPORTANT:** If you see an error like `AttributeError: partially initialized module 'torchvision'` or `ModuleNotFoundError: PreTrainedModel`, you **MUST**:\n",
    "1. Run the cell below.\n",
    "2. **Restart the Runtime** (Runtime -> Restart session).\n",
    "3. Run the valid imports cell (skip the install cell if you just ran it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:342dbf1b-1373-41c8-b34b-fe1033b9a21b"
     ]
    }
   ],
   "source": [
    "# 1. Uninstall potentially conflicting versions first\n",
    "!pip uninstall -y torchvision torch torchaudio\n",
    "\n",
    "# 2. Install compatible Torch + Torchvision\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# 3. Install other dependencies (including python-dotenv for local env)\n",
    "!pip install -q -U transformers peft trl bitsandbytes accelerate datasets scipy matplotlib python-dotenv\n",
    "\n",
    "print(\"Installation complete. PLEASE RESTART RUNTIME now if this is the first run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Environment Check\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seeds_header"
   },
   "source": [
    "## 2. strict Reproducibility (Seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "set_seeds"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# Set environment variable for Python hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Set seeds\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # Note: These settings may impact performance\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\" Seeds set to {SEED} for reproducibility\")\n",
    "print(\" Note: Full determinism on GPU is not guaranteed due to non-deterministic operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "path_config_header"
   },
   "source": [
    "## 3. Path Configuration & Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "path_config_cell"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Define the User's Local Path\n",
    "USER_LOCAL_ROOT = r\"C:/Development/financial-intelligence-engine\"\n",
    "USER_LOCAL_DATA = os.path.join(USER_LOCAL_ROOT, \"data/processed\")\n",
    "USER_LOCAL_OUTPUT = os.path.join(USER_LOCAL_ROOT, \"data/results\")\n",
    "\n",
    "# 2. Check Environment\n",
    "if os.path.exists(USER_LOCAL_ROOT):\n",
    "    # Running Locally (Windows/VS Code Local Kernel)\n",
    "    print(\" Local Windows Environment Detected.\")\n",
    "    BASE_PATH = USER_LOCAL_ROOT\n",
    "    DATA_PATH = USER_LOCAL_DATA\n",
    "    OUTPUT_PATH = USER_LOCAL_OUTPUT\n",
    "    \n",
    "elif 'google.colab' in sys.modules:\n",
    "    # Running in Colab\n",
    "    print(\" Google Colab Environment Detected.\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Define Drive Structure\n",
    "    DRIVE_ROOT = \"/content/drive/MyDrive/Financial_Intern_Project\"\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_ROOT}/data\", exist_ok=True)\n",
    "    os.makedirs(f\"{DRIVE_ROOT}/results\", exist_ok=True)\n",
    "    \n",
    "    BASE_PATH = DRIVE_ROOT\n",
    "    DATA_PATH = f\"{DRIVE_ROOT}/data/processed\"\n",
    "    OUTPUT_PATH = f\"{DRIVE_ROOT}/results\"\n",
    "    \n",
    "    print(f\" Google Drive Mounted at: {BASE_PATH}\")\n",
    "    print(f\" Please upload 'train.jsonl' to: {DATA_PATH}\")\n",
    "    print(f\"Results will be saved to: {OUTPUT_PATH}\")\n",
    "    \n",
    "elif os.path.exists(\"/mnt/c\"):\n",
    "    # WSL\n",
    "    print(\" WSL Detected.\")\n",
    "    BASE_PATH = \"/mnt/c/Development/financial-intelligence-engine\"\n",
    "    DATA_PATH = f\"{BASE_PATH}/data/processed\"\n",
    "    OUTPUT_PATH = f\"{BASE_PATH}/data/results\"\n",
    "else:\n",
    "    # Generic Local / Repo Root\n",
    "    print(\"Generic Environment. Using relative paths.\")\n",
    "    BASE_PATH = \".\"\n",
    "    DATA_PATH = \"../data/processed\"\n",
    "    OUTPUT_PATH = \"../data/results\"\n",
    "\n",
    "print(f\" Data Path: {DATA_PATH}\")\n",
    "print(f\" Output Path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_header"
   },
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "config_cell"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Model\n",
    "    # \"model_id\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"model_id\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"new_model_name\": \"Llama-3-8b-Financial-Intern\",\n",
    "    \"hf_username\": \"p-sahas\", \n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_r\": 64,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 1,\n",
    "    \"grad_accumulation\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"max_steps\": 150,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 50,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"output_dir\": OUTPUT_PATH, # Uses dynamic path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auth_header"
   },
   "source": [
    "## 5. Hugging Face Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hf_auth"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Attempt to load .env file if available (local usage)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, skipping .env loading (pip install python-dotenv)\")\n",
    "\n",
    "try:\n",
    "    # 1. Try Environment Variable (from .env or system)\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    \n",
    "    # 2. If missing, Try Colab Secrets\n",
    "    if not token:\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            token = userdata.get('HF_TOKEN')\n",
    "        except ImportError:\n",
    "            pass # Not in Colab\n",
    "        except Exception as e:\n",
    "            print(f\"Colab secret fetch skipped: {e}\")\n",
    "\n",
    "    # 3. Login or Fallback\n",
    "    if token:\n",
    "        print(f\"Logging in with token: {token[:4]}...{token[-4:]}\")\n",
    "        login(token)\n",
    "    else:\n",
    "        raise ValueError(\"HF_TOKEN not found in environment variable or Colab secrets.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Fallback to interactive login\n",
    "    print(f\"Automatic login failed: {e}\")\n",
    "    print(\"Falling back to manual login...\")\n",
    "    login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_header"
   },
   "source": [
    "## 6. Data Loading & Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "data_load"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Use the dynamic DATA_PATH configured earlier\n",
    "dataset_filename = \"train.jsonl\"\n",
    "possible_paths = [\n",
    "    r\"c:/Development/financial-intelligence-engine/data/processed/train.jsonl\", # Absolute path (Highest Priority)\n",
    "    os.path.join(DATA_PATH, dataset_filename),  # Path from Config (Local/Drive)\n",
    "    \"train.jsonl\",                              # Copy in CWD\n",
    "    \"../data/processed/train.jsonl\",            # Relative fallback\n",
    "    \"/content/train.jsonl\"                      # Colab root\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        dataset_path = path\n",
    "        break\n",
    "\n",
    "if not dataset_path:\n",
    "    print(f\" Dataset not found in known locations. Searched: {possible_paths}\")\n",
    "    raise FileNotFoundError(\"CRITICAL: Dataset 'train.jsonl' not found. Cannot proceed.\")\n",
    "\n",
    "print(f\"Using dataset path: {dataset_path}\")\n",
    "try:\n",
    "    train_dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "    print(f\" Loaded {len(train_dataset)} training examples.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load dataset: {e}\")\n",
    "\n",
    "def format_instruction(sample):\n",
    "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on context.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{sample['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{sample['answer']}<|eot_id|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_header"
   },
   "source": [
    "## 7. Model Loading (4-Bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_load"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# 1. Clear Cache before loading\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")\n",
    "\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading model (4-bit)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_id\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",         # <--- Changed to auto for better dispatch\n",
    "    offload_buffers=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_cache=False,\n",
    ")\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_id\"], trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_header"
   },
   "source": [
    "## 8. Training & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=CONFIG[\"target_modules\"]\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"grad_accumulation\"],\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=CONFIG[\"max_steps\"],\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=format_instruction,\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loss_plot"
   },
   "outputs": [],
   "source": [
    "# Visualize Training Loss\n",
    "history = trainer.state.log_history\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "for entry in history:\n",
    "    if 'loss' in entry:\n",
    "        steps.append(entry['step'])\n",
    "        losses.append(entry['loss'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_header"
   },
   "source": [
    "## 9. Save, Model Card & Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_card"
   },
   "outputs": [],
   "source": [
    "# Create Model Card (README.md)\n",
    "model_card_content = f\"\"\"\n",
    "---\n",
    "base_model: {CONFIG['model_id']}\n",
    "library_name: peft\n",
    "license: mit\n",
    "tags:\n",
    "- finance\n",
    "- llama-3\n",
    "- 4-bit\n",
    "- loRA\n",
    "---\n",
    "\n",
    "# {CONFIG['new_model_name']}\n",
    "\n",
    "This model is a fine-tuned version of **{CONFIG['model_id']}** on the **Uber 2024 Annual Report** dataset. \n",
    "It is designed to act as a financial analyst \"Intern\", capable of answering questions with the specific strategy and tone of the source document.\n",
    "\n",
    "## Model Details\n",
    "- **Base Model:** {CONFIG['model_id']}\n",
    "- **Architecture:** 4-bit Quantization (NF4) + LoRA Adapters\n",
    "- **Task:** Financial Question Answering\n",
    "\n",
    "## Training Configuration\n",
    "- **LoRA Rank:** {CONFIG['lora_r']}\n",
    "- **LoRA Alpha:** {CONFIG['lora_alpha']}\n",
    "- **Batch Size:** {CONFIG['batch_size']}\n",
    "- **Learning Rate:** {CONFIG['learning_rate']}\n",
    "- **Epochs:** {CONFIG['num_epochs']}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "base_model_id = \"{CONFIG['model_id']}\"\n",
    "adapter_model_id = \"{CONFIG['hf_username']}/{CONFIG['new_model_name']}\"\n",
    "\n",
    "# Load Base Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
    "# Load Adapter\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "input_text = \"What is the company's strategy for 2024?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(CONFIG['new_model_name'], exist_ok=True)\n",
    "\n",
    "with open(f\"{CONFIG['new_model_name']}/README.md\", \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "\n",
    "print(f\"Model Card saved to {CONFIG['new_model_name']}/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "new_model = CONFIG[\"new_model_name\"]\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "# trainer.model.push_to_hub(new_model, use_temp_dir=False)\n",
    "# tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval_header"
   },
   "source": [
    "## 10. Extended Evaluation (Golden Test Set)\n",
    "Evaluating on the `golden_test_set.jsonl` and saving detailed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_func"
   },
   "outputs": [],
   "source": [
    "def query_intern(question, model_to_use, tokenizer_to_use):\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on context.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    \n",
    "    inputs = tokenizer_to_use(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_to_use.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_to_use.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer_to_use.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"assistant\")[-1].strip()\n",
    "\n",
    "# Load Golden Test Set\n",
    "test_dataset = load_dataset(\"json\", data_files=r\"c:/Development/financial-intelligence-engine/data/processed/golden_test_set.jsonl\", split=\"train\")\n",
    "print(f\"Loaded {len(test_dataset)} test examples.\")\n",
    "\n",
    "# Evaluate on a subset (e.g., first 10 for demonstration, or all)\n",
    "results = []\n",
    "print(\"Running evaluation...\")\n",
    "\n",
    "# Load Base Model for comparison\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CONFIG[\"model_id\"],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    if i >= 10: break # Limit to 10 for speed in notebook demo, remove for full run\n",
    "    \n",
    "    question = sample['question']\n",
    "    ground_truth = sample['answer']\n",
    "    \n",
    "    # Base Model Prediction\n",
    "    base_pred = query_intern(question, base_model, tokenizer)\n",
    "    \n",
    "    # Fine-Tuned Prediction\n",
    "    tuned_pred = query_intern(question, trainer.model, tokenizer)\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"base_model\": base_pred,\n",
    "        \"fine_tuned\": tuned_pred\n",
    "    })\n",
    "\n",
    "# Print Comparison\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"\\n--- Visual Comparison (Top 5) ---\")\n",
    "display(df_results.head(5)) # Use display() in Jupyter, or print() otherwise\n",
    "\n",
    "# Save Results\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "df_results.to_json(\"../results/finetuned_results.jsonl\", orient=\"records\", lines=True)\n",
    "print(\"\\nResults saved to ../results/finetuned_results.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
