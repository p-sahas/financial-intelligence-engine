{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Project Setup Verification\n",
                "\n",
                "This notebook verifies that the environment is set up correctly, including:\n",
                "1. **Dependencies**: Checks if required libraries are installed.\n",
                "2. **Data Paths**: Confirms that data files exist in `data/processed`.\n",
                "3. **LLM Fallback**: Tests the robustness of the LLM configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Project Root: C:\\Development\\financial-intelligence-engine\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Add src to path\n",
                "project_root = Path(\"..\").resolve()\n",
                "sys.path.append(str(project_root / \"src\"))\n",
                "\n",
                "print(f\"Project Root: {project_root}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Check Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SUCCESS: Critical dependencies imported.\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    import langchain\n",
                "    import langchain_core\n",
                "    import langchain_openai\n",
                "    from services.llm_services import get_llm\n",
                "    print(\"SUCCESS: Critical dependencies imported.\")\n",
                "except ImportError as e:\n",
                "    print(f\"FAILURE: Missing dependency: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Check Data Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SUCCESS: Found train.jsonl\n",
                        "SUCCESS: Found golden_test_set.jsonl\n",
                        "SUCCESS: Found qa_dataset_full.json\n",
                        "\n",
                        "All data files present in 'data/processed'.\n"
                    ]
                }
            ],
            "source": [
                "processed_dir = project_root / \"data/processed\"\n",
                "files_to_check = [\"train.jsonl\", \"golden_test_set.jsonl\", \"qa_dataset_full.json\"]\n",
                "\n",
                "all_exist = True\n",
                "for filename in files_to_check:\n",
                "    path = processed_dir / filename\n",
                "    if path.exists():\n",
                "        print(f\"SUCCESS: Found {filename}\")\n",
                "    else:\n",
                "        print(f\"FAILURE: Missing {filename} at {path}\")\n",
                "        all_exist = False\n",
                "\n",
                "if all_exist:\n",
                "    print(\"\\nAll data files present in 'data/processed'.\")\n",
                "else:\n",
                "    print(\"\\nSome data files are missing. Please run '01_data_factory.ipynb' first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Verify LLM Fallback Logic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Testing Fallback Configuration Construction...\n",
                        "Check: 'temperature'\n"
                    ]
                }
            ],
            "source": [
                "from langchain_core.runnables import RunnableWithFallbacks\n",
                "\n",
                "print(\"Testing Fallback Configuration Construction...\")\n",
                "\n",
                "# Mock Config with Multi-Provider Fallback\n",
                "config = {\n",
                "    \"llm_provider\": \"openai\",\n",
                "    \"llm_model\": \"gpt-4o\",\n",
                "    \"openai_api_key\": \"sk-fake-key\", # Fake key, we only test construction\n",
                "    \"enable_fallback\": True,\n",
                "    \"fallbacks\": [\n",
                "        {\"llm_provider\": \"openai\", \"llm_model\": \"gpt-3.5-turbo\"},\n",
                "        {\"llm_provider\": \"openai\", \"llm_model\": \"gpt-4-turbo\"}\n",
                "    ]\n",
                "}\n",
                "\n",
                "try:\n",
                "    # force a simplistic creation to avoid actual API calls if possible,\n",
                "    # but get_llm might try to validate. \n",
                "    # If it fails due to API key, we catch it.\n",
                "    # However, langchain usually lazy-initializes, so construction might pass.\n",
                "    llm = get_llm(config)\n",
                "    \n",
                "    if isinstance(llm, RunnableWithFallbacks):\n",
                "        print(\"SUCCESS: LLM instance is correctly wrapped with RunnableWithFallbacks\")\n",
                "        print(f\"Primary Model: {llm.runnable.model_name if hasattr(llm.runnable, 'model_name') else llm.runnable}\")\n",
                "        print(f\"Fallback Models Count: {len(llm.fallbacks)}\")\n",
                "        if len(llm.fallbacks) == 2:\n",
                "             print(\"SUCCESS: Correct number of fallbacks (2) instantiated.\")\n",
                "        else:\n",
                "             print(f\"FAILURE: Expected 2 fallbacks, got {len(llm.fallbacks)}\")\n",
                "    else:\n",
                "        print(\"FAILURE: LLM instance is NOT wrapped with RunnableWithFallbacks\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Check: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "sahas",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
