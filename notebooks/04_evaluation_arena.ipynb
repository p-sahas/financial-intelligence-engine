{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 4: The Showdown (Evaluation Arena)\n",
                "\n",
                "**Objective:** Compare \"The Librarian\" (RAG System) vs. \"The Intern\" (Fine-Tuned Model) on the Golden Test Set.\n",
                "\n",
                "**Metrics:**\n",
                "1.  **ROUGE-L**: Measures text overlap (Precision, Recall, F1) against Ground Truth.\n",
                "2.  **LLM-as-a-Judge**: Uses a superior model (e.g., GPT-4o) to score answer quality (1-5) and reasoning.\n",
                "3.  **Latency**: Time taken to generate the answer.\n",
                "4.  **Cost**: Estimated cost per 1k queries.\n",
                "\n",
                "**Note:** Since the fine-tuning step was skipped/mocked, \"The Intern\" evaluation will use the **Base Model** (Llama-3-8B) as a proxy to demonstrate the pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies (if needed)\n",
                "!pip install -q rouge_score weave weaviate-client langchain langchain-community langchain-huggingface sentence-transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Imports & Configuration\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "import yaml\n",
                "import pandas as pd\n",
                "import weaviate\n",
                "from rouge_score import rouge_scorer\n",
                "\n",
                "# Add src to path\n",
                "import sys\n",
                "sys.path.append(os.path.abspath(\"../src\"))\n",
                "\n",
                "from services.llm_services import get_llm\n",
                "from utils.cost_tracker import get_token_count, PRICING\n",
                "\n",
                "# Load Config\n",
                "with open(\"../src/config/config.yaml\", \"r\") as f:\n",
                "    config = yaml.safe_load(f)\n",
                "\n",
                "# Paths\n",
                "GOLDEN_SET_PATH = config[\"data\"][\"golden_test_set_path\"] # e.g. data/processed/golden_test_set.jsonl\n",
                "RESULTS_PATH = config[\"data\"][\"eval_results_path\"] # e.g. data/results/rag_evaluation_results.json\n",
                "\n",
                "print(f\"Config Loaded. Testing on: {GOLDEN_SET_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Metric Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_rouge(reference, candidate):\n",
                "    \"\"\"Calculates ROUGE-L score.\"\"\"\n",
                "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
                "    scores = scorer.score(reference, candidate)\n",
                "    return scores['rougeL'].fmeasure\n",
                "\n",
                "def calculate_cost(input_text, output_text, model_name):\n",
                "    \"\"\"Estimates cost based on token counts and PRICING.\"\"\"\n",
                "    in_tokens = get_token_count(input_text)\n",
                "    out_tokens = get_token_count(output_text)\n",
                "    \n",
                "    # Normalize model name key (handle variations)\n",
                "    pricing = PRICING.get(model_name, PRICING.get(\"openai/gpt-4o-mini\"))\n",
                "    \n",
                "    cost = (in_tokens / 1_000_000) * pricing[\"input\"] + (out_tokens / 1_000_000) * pricing[\"output\"]\n",
                "    return cost\n",
                "\n",
                "def llm_judge(question, ground_truth, answer, judge_model=None):\n",
                "    \"\"\"Uses an LLM to grade the answer 1-5.\"\"\"\n",
                "    if judge_model is None:\n",
                "        # Initialize a strong judge model (e.g. GPT-4o or similar)\n",
                "        judge_config = config.copy()\n",
                "        judge_config[\"llm_model\"] = \"gpt-4o\" # Force strong model if available, or fall back to config default\n",
                "        judge_model = get_llm(judge_config)\n",
                "\n",
                "    prompt_template = \"\"\"\n",
                "    You are an impartial judge evaluating a financial analyst's answer.\n",
                "    \n",
                "    Question: {question}\n",
                "    Ground Truth: {ground_truth}\n",
                "    Student Answer: {answer}\n",
                "    \n",
                "    Evaluate the Student Answer based on accuracy, completeness, and tone compared to the Ground Truth.\n",
                "    Output a JSON object with two keys:\n",
                "    - \"score\": an integer 1-5 (1=bad, 5=excellent)\n",
                "    - \"reasoning\": a brief explanation.\n",
                "    Do not output markdown formatting, just the raw JSON.\n",
                "    \"\"\"\n",
                "    \n",
                "    try:\n",
                "        response = judge_model.invoke(prompt_template.format(question=question, ground_truth=ground_truth, answer=answer))\n",
                "        content = response.content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
                "        result = json.loads(content)\n",
                "        return result.get(\"score\", 0), result.get(\"reasoning\", \"Parse Error\")\n",
                "    except Exception as e:\n",
                "        print(f\"Judge Error: {e}\")\n",
                "        return 0, \"Error\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. System 1: \"The Librarian\" (RAG Setup)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from sentence_transformers import CrossEncoder\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "\n",
                "# --- 1. Connect to Weaviate ---\n",
                "# Try WCS first, then Local\n",
                "try:\n",
                "    wcs_url = config[\"vectordb\"].get(\"wcs_url\")\n",
                "    wcs_api_key = config[\"vectordb\"].get(\"wcs_api_key\")\n",
                "    if wcs_url and wcs_api_key:\n",
                "        print(\"Connecting to Weaviate Cloud...\")\n",
                "        client = weaviate.connect_to_wcs(\n",
                "            cluster_url=wcs_url,\n",
                "            auth_credentials=weaviate.auth.AuthApiKey(wcs_api_key)\n",
                "        )\n",
                "    else:\n",
                "        print(\"Connecting to Weaviate Local...\")\n",
                "        client = weaviate.connect_to_local()\n",
                "    print(f\"Weaviate Connected. Ready: {client.is_ready()}\")\n",
                "except Exception as e:\n",
                "    print(f\"Weaviate Connection Failed: {e}\")\n",
                "\n",
                "# --- 2. Embeddings & Reranker ---\n",
                "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
                "\n",
                "# --- 3. Retrieval Functions ---\n",
                "def hybrid_search(query, limit=20):\n",
                "    collection = client.collections.get(\"FinancialReport\")\n",
                "    query_vector = embedding_model.embed_query(query)\n",
                "    response = collection.query.hybrid(\n",
                "        query=query,\n",
                "        vector=query_vector,\n",
                "        alpha=0.5,\n",
                "        limit=limit,\n",
                "        return_metadata=weaviate.classes.query.MetadataQuery(score=True)\n",
                "    )\n",
                "    results = []\n",
                "    for o in response.objects:\n",
                "        res = o.properties\n",
                "        res['score'] = o.metadata.score\n",
                "        results.append(res)\n",
                "    return results\n",
                "\n",
                "def rerank_results(query, retrieved_docs, top_k=5):\n",
                "    if not retrieved_docs: return []\n",
                "    pairs = [[query, doc['text']] for doc in retrieved_docs]\n",
                "    scores = reranker.predict(pairs)\n",
                "    for i, doc in enumerate(retrieved_docs):\n",
                "        doc['rerank_score'] = float(scores[i])\n",
                "    return sorted(retrieved_docs, key=lambda x: x['rerank_score'], reverse=True)[:top_k]\n",
                "\n",
                "def format_docs(docs):\n",
                "    return \"\\n\\n\".join([f\"[Source: Page {d.get('page_number', '?')}] {d['text']}\" for d in docs])\n",
                "\n",
                "# --- 4. RAG Chain ---\n",
                "rag_template = \"\"\"\n",
                "You are a specialized financial analyst assistant.\n",
                "Use the following context to answer the user's question accurately.\n",
                "If the answer is not in the context, say \"I don't have enough information.\"\n",
                "Keep answers professional and concise.\n",
                "\n",
                "Context:\n",
                "{context}\n",
                "\n",
                "Question: {question}\n",
                "Answer:\n",
                "\"\"\"\n",
                "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
                "rag_llm = get_llm(config) # Uses default llm_model from config\n",
                "rag_chain = rag_prompt | rag_llm | StrOutputParser()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. System 2: \"The Intern\" (Mock Fine-Tuned Model)\n",
                "Since we didn't perform the physical fine-tuning, we will use the **Base Model** directly as a proxy. Realistically, this would be the `PeftModel` loaded from disk."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "intern_template = \"\"\"\n",
                "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on your internal knowledge and the following question.\n",
                "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "{question}\n",
                "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
                "\"\"\"\n",
                "\n",
                "intern_prompt = ChatPromptTemplate.from_template(intern_template)\n",
                "intern_llm = get_llm(config)\n",
                "intern_chain = intern_prompt | intern_llm | StrOutputParser()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Run Evaluation Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Test Set\n",
                "test_set = []\n",
                "with open(GOLDEN_SET_PATH, 'r') as f:\n",
                "    for line in f:\n",
                "        test_set.append(json.loads(line))\n",
                "\n",
                "print(f\"Loaded {len(test_set)} test questions.\")\n",
                "\n",
                "results = []\n",
                "\n",
                "# Define Judge Model once\n",
                "judge_config = config.copy()\n",
                "judge_config[\"llm_model\"] = \"gpt-4o\" # Or \"gpt-4-turbo\", \"claude-3-opus\", etc.\n",
                "judge_config[\"llm_provider\"] = \"openai\" # Ensure provider matches\n",
                "# Fallback if user doesn't have GPT-4o keys configured: use the default config's model\n",
                "try:\n",
                "    judge_bot = get_llm(judge_config)\n",
                "except:\n",
                "    print(\"Warning: Judge model init failed, using default config.\")\n",
                "    judge_bot = get_llm(config)\n",
                "\n",
                "for i, sample in enumerate(test_set):\n",
                "    q = sample['question']\n",
                "    gt = sample['answer']\n",
                "    \n",
                "    print(f\"Evaluating Q{i+1}/{len(test_set)}...\")\n",
                "    \n",
                "    # --- 1. Evaluator: Librarian (RAG) ---\n",
                "    start = time.perf_counter()\n",
                "    # Retrieval\n",
                "    retrieved = hybrid_search(q)\n",
                "    reranked = rerank_results(q, retrieved)\n",
                "    context_str = format_docs(reranked)\n",
                "    # Generation\n",
                "    rag_response = rag_chain.invoke({\"context\": context_str, \"question\": q})\n",
                "    rag_time = time.perf_counter() - start\n",
                "    \n",
                "    # --- 2. Evaluator: Intern (Mock) ---\n",
                "    start = time.perf_counter()\n",
                "    intern_response = intern_chain.invoke({\"question\": q})\n",
                "    intern_time = time.perf_counter() - start\n",
                "    \n",
                "    # --- 3. Scoring ---\n",
                "    \n",
                "    # ROUGE\n",
                "    rag_rouge = calculate_rouge(gt, rag_response)\n",
                "    intern_rouge = calculate_rouge(gt, intern_response)\n",
                "    \n",
                "    # Judge\n",
                "    rag_score, rag_reason = llm_judge(q, gt, rag_response, judge_bot)\n",
                "    intern_score, intern_reason = llm_judge(q, gt, intern_response, judge_bot)\n",
                "    \n",
                "    # Cost (Est.)\n",
                "    rag_cost = calculate_cost(context_str + q, rag_response, config[\"llm_model\"])\n",
                "    intern_cost = calculate_cost(q, intern_response, config[\"llm_model\"])\n",
                "    \n",
                "    results.append({\n",
                "        \"question\": q,\n",
                "        \"ground_truth\": gt,\n",
                "        \"librarian_answer\": rag_response,\n",
                "        \"intern_answer\": intern_response,\n",
                "        \"librarian_time\": rag_time,\n",
                "        \"intern_time\": intern_time,\n",
                "        \"librarian_rouge\": rag_rouge,\n",
                "        \"intern_rouge\": intern_rouge,\n",
                "        \"librarian_score\": rag_score,\n",
                "        \"intern_score\": intern_score,\n",
                "        \"librarian_cost\": rag_cost,\n",
                "        \"intern_cost\": intern_cost,\n",
                "        \"librarian_judge_reason\": rag_reason,\n",
                "        \"intern_judge_reason\": intern_reason\n",
                "    })\n",
                "\n",
                "# Save Results\n",
                "df_res = pd.DataFrame(results)\n",
                "os.makedirs(\"../data/results\", exist_ok=True)\n",
                "df_res.to_json(RESULTS_PATH, orient=\"records\", indent=2)\n",
                "print(f\"Evaluation Complete. Results saved to {RESULTS_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Results & Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary Table\n",
                "summary = df_res[[\n",
                "    \"librarian_time\", \"intern_time\", \n",
                "    \"librarian_rouge\", \"intern_rouge\", \n",
                "    \"librarian_score\", \"intern_score\",\n",
                "    \"librarian_cost\", \"intern_cost\"\n",
                "]].mean()\n",
                "\n",
                "print(\"--- Average Metrics ---\")\n",
                "print(summary)\n",
                "\n",
                "# Detailed View\n",
                "display(df_res[[\"question\", \"librarian_score\", \"intern_score\", \"librarian_rouge\", \"intern_rouge\"]].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Business Cost Analysis\n",
                "**Scenario**: 500 Daily Users, 10 Queries each = 5,000 queries/day."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DAILY_QUERIES = 5000\n",
                "\n",
                "rag_daily_cost = summary[\"librarian_cost\"] * DAILY_QUERIES\n",
                "intern_daily_cost = summary[\"intern_cost\"] * DAILY_QUERIES\n",
                "\n",
                "print(f\"--- ROI / Cost Analysis (Per Day) ---\")\n",
                "print(f\"The Librarian (RAG) Cost: ${rag_daily_cost:.2f}\")\n",
                "print(f\"The Intern (Fine-Tuned) Cost: ${intern_daily_cost:.2f}\")\n",
                "\n",
                "diff = rag_daily_cost - intern_daily_cost\n",
                "if diff > 0:\n",
                "    print(f\"Fine-Tuned Model saves ${diff:.2f} per day (${diff*30:.2f}/month).\")\n",
                "else:\n",
                "    print(f\"RAG System saves ${abs(diff):.2f} per day (${abs(diff)*30:.2f}/month).\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "sahas",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
