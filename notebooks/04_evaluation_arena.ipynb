{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 4: The Showdown (Evaluation Arena)\n",
                "\n",
                "**Objective:** Compare \"The Librarian\" (RAG System) vs. \"The Intern\" (Fine-Tuned Model) on the Golden Test Set.\n",
                "\n",
                "**Metrics:**\n",
                "1.  **ROUGE-L**: Measures text overlap (Precision, Recall, F1) against Ground Truth.\n",
                "2.  **LLM-as-a-Judge**: Uses a superior model (e.g., GPT-4o) to score answer quality (1-5) and reasoning.\n",
                "3.  **Latency**: Time taken to generate the answer.\n",
                "4.  **Cost**: Estimated cost per 1k queries.\n",
                "\n",
                "**Note:** Since the fine-tuning step was skipped/mocked, \"The Intern\" evaluation will use the **Base Model** (Llama-3-8B) as a proxy to demonstrate the pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies (if needed)\n",
                "# !pip install -q rouge_score weave weaviate-client langchain langchain-community langchain-huggingface sentence-transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Config Loaded (partial). Testing on: ../data/processed/golden_test_set.jsonl\n"
                    ]
                }
            ],
            "source": [
                "# 2. Imports & Configuration\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "import yaml\n",
                "import pandas as pd\n",
                "import weaviate\n",
                "from rouge_score import rouge_scorer\n",
                "\n",
                "# Add src to path\n",
                "import sys\n",
                "sys.path.append(os.path.abspath(\"../src\"))\n",
                "\n",
                "from services.llm_services import get_llm\n",
                "from utils.cost_tracker import get_token_count, PRICING\n",
                "\n",
                "# Load Config (safe)\n",
                "config = {}\n",
                "try:\n",
                "    with open(\"../src/config/config.yaml\", \"r\") as f:\n",
                "        config = yaml.safe_load(f) or {}\n",
                "except FileNotFoundError:\n",
                "    print(\"Warning: ../src/config/config.yaml not found. Using defaults.\")\n",
                "except Exception as e:\n",
                "    print(f\"Warning: Error loading config: {e}. Using defaults.\")\n",
                "\n",
                "# Paths with fallbacks to avoid KeyError if 'data' is missing in config\n",
                "data_cfg = config.get(\"data\", {})\n",
                "GOLDEN_SET_PATH = data_cfg.get(\"golden_test_set_path\", '../data/processed/golden_test_set.jsonl')\n",
                "RESULTS_PATH = data_cfg.get(\"eval_results_path\", '../data/results/rag_evaluation_results.json')\n",
                "\n",
                "print(f\"Config Loaded (partial). Testing on: {GOLDEN_SET_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Metric Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_rouge(reference, candidate):\n",
                "    \"\"\"Calculates ROUGE-L score.\"\"\"\n",
                "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
                "    scores = scorer.score(reference, candidate)\n",
                "    return scores['rougeL'].fmeasure\n",
                "\n",
                "def calculate_cost(input_text, output_text, model_name):\n",
                "    \"\"\"Estimates cost based on token counts and PRICING.\"\"\"\n",
                "    in_tokens = get_token_count(input_text)\n",
                "    out_tokens = get_token_count(output_text)\n",
                "    \n",
                "    # Normalize model name key (handle variations)\n",
                "    pricing = PRICING.get(model_name, PRICING.get(\"openai/gpt-4o-mini\"))\n",
                "    \n",
                "    cost = (in_tokens / 1_000_000) * pricing[\"input\"] + (out_tokens / 1_000_000) * pricing[\"output\"]\n",
                "    return cost\n",
                "\n",
                "def llm_judge(question, ground_truth, answer, judge_model=None):\n",
                "    \"\"\"Uses an LLM to grade the answer 1-5.\"\"\"\n",
                "    if judge_model is None:\n",
                "        # Initialize a strong judge model (e.g. GPT-4o or similar)\n",
                "        judge_config = config.copy()\n",
                "        judge_config[\"llm_model\"] = \"gpt-4o\" # Force strong model if available, or fall back to config default\n",
                "        judge_model = get_llm(judge_config)\n",
                "\n",
                "    prompt_template = \"\"\"\n",
                "    You are an impartial judge evaluating a financial analyst's answer.\n",
                "    \n",
                "    Question: {question}\n",
                "    Ground Truth: {ground_truth}\n",
                "    Student Answer: {answer}\n",
                "    \n",
                "    Evaluate the Student Answer based on accuracy, completeness, and tone compared to the Ground Truth.\n",
                "    Output a JSON object with two keys:\n",
                "    - \"score\": an integer 1-5 (1=bad, 5=excellent)\n",
                "    - \"reasoning\": a brief explanation.\n",
                "    Do not output markdown formatting, just the raw JSON.\n",
                "    \"\"\"\n",
                "    \n",
                "    try:\n",
                "        response = judge_model.invoke(prompt_template.format(question=question, ground_truth=ground_truth, answer=answer))\n",
                "        content = response.content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
                "        result = json.loads(content)\n",
                "        return result.get(\"score\", 0), result.get(\"reasoning\", \"Parse Error\")\n",
                "    except Exception as e:\n",
                "        print(f\"Judge Error: {e}\")\n",
                "        return 0, \"Error\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. System 1: \"The Librarian\" (RAG Setup)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:From c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
                        "\n",
                        "1. Trying Local Docker...\n",
                        " Connected to Local Docker!\n",
                        "Weaviate Ready: True\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from sentence_transformers import CrossEncoder\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "\n",
                "# --- 1. Connect to Weaviate ---\n",
                "client = None\n",
                "\n",
                "# 1. Try Local Docker (Preferred for Windows)\n",
                "print(\"1. Trying Local Docker...\")\n",
                "try:\n",
                "    client = weaviate.connect_to_local()\n",
                "    if client.is_ready():\n",
                "        print(\" Connected to Local Docker!\")\n",
                "except Exception as e:\n",
                "    print(f\" Local connection failed: {e}\")\n",
                "    client = None\n",
                "\n",
                "# 2. Try Cloud (WCS) if Local failed\n",
                "if not client:\n",
                "    print(\"2. Trying Weaviate Cloud...\")\n",
                "    try:\n",
                "        # Fix: Safely get config, fallback to env vars\n",
                "        vectordb_cfg = config.get(\"vector_db\", config.get(\"vectordb\", {}))\n",
                "        wcs_url = vectordb_cfg.get(\"wcs_url\") or os.environ.get(\"WEAVIATE_URL\")\n",
                "        wcs_api_key = vectordb_cfg.get(\"wcs_api_key\") or os.environ.get(\"WEAVIATE_API_KEY\")\n",
                "        \n",
                "        if wcs_url and wcs_api_key:\n",
                "            client = weaviate.connect_to_wcs(\n",
                "                cluster_url=wcs_url,\n",
                "                auth_credentials=weaviate.auth.AuthApiKey(wcs_api_key)\n",
                "            )\n",
                "            print(\" Connected to Weaviate Cloud!\")\n",
                "        else:\n",
                "            print(\" No Cloud credentials found.\")\n",
                "    except Exception as e:\n",
                "        print(f\" Cloud connection failed: {e}\")\n",
                "\n",
                "if not client or not client.is_ready():\n",
                "    print(\"CRITICAL: Weaviate connection failed.\")\n",
                "else:\n",
                "    print(f\"Weaviate Ready: {client.is_ready()}\")\n",
                "\n",
                "# --- 2. Embeddings & Reranker ---\n",
                "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
                "\n",
                "# --- 3. Retrieval Functions ---\n",
                "def hybrid_search(query, limit=20):\n",
                "    if not client:\n",
                "        raise RuntimeError(\"Weaviate client is not connected.\")\n",
                "    \n",
                "    # FIX: Get collection name from config\n",
                "    vectordb_cfg = config.get(\"vector_db\", config.get(\"vectordb\", {}))\n",
                "    collection_name = vectordb_cfg.get(\"collection_name\", \"FinancialReport\")\n",
                "    \n",
                "    collection = client.collections.get(collection_name)\n",
                "    query_vector = embedding_model.embed_query(query)\n",
                "    response = collection.query.hybrid(\n",
                "        query=query,\n",
                "        vector=query_vector,\n",
                "        alpha=0.5,\n",
                "        limit=limit,\n",
                "        return_metadata=weaviate.classes.query.MetadataQuery(score=True)\n",
                "    )\n",
                "    results = []\n",
                "    for o in response.objects:\n",
                "        res = o.properties\n",
                "        res['score'] = o.metadata.score\n",
                "        results.append(res)\n",
                "    return results\n",
                "\n",
                "def rerank_results(query, retrieved_docs, top_k=5):\n",
                "    if not retrieved_docs: return []\n",
                "    pairs = [[query, doc['text']] for doc in retrieved_docs]\n",
                "    scores = reranker.predict(pairs)\n",
                "    for i, doc in enumerate(retrieved_docs):\n",
                "        doc['rerank_score'] = float(scores[i])\n",
                "    return sorted(retrieved_docs, key=lambda x: x['rerank_score'], reverse=True)[:top_k]\n",
                "\n",
                "def format_docs(docs):\n",
                "    return \"\\n\\n\".join([f\"[Source: Page {d.get('page_number', '?')}] {d['text']}\" for d in docs])\n",
                "\n",
                "# --- 4. RAG Chain ---\n",
                "rag_template = \"\"\"\n",
                "You are a specialized financial analyst assistant.\n",
                "Use the following context to answer the user's question accurately.\n",
                "If the answer is not in the context, say \"I don't have enough information.\"\n",
                "Keep answers professional and concise.\n",
                "\n",
                "Context:\n",
                "{context}\n",
                "\n",
                "Question: {question}\n",
                "Answer:\n",
                "\"\"\"\n",
                "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
                "rag_llm = get_llm(config) # Uses default llm_model from config\n",
                "rag_chain = rag_prompt | rag_llm | StrOutputParser()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. System 2: \"The Intern\" (Mock Fine-Tuned Model)\n",
                "Since we didn't perform the physical fine-tuning, we will use the **Base Model** directly as a proxy. Realistically, this would be the `PeftModel` loaded from disk."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "intern_template = \"\"\"\n",
                "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on your internal knowledge and the following question.\n",
                "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "{question}\n",
                "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
                "\"\"\"\n",
                "\n",
                "intern_prompt = ChatPromptTemplate.from_template(intern_template)\n",
                "intern_llm = get_llm(config)\n",
                "intern_chain = intern_prompt | intern_llm | StrOutputParser()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Run Evaluation Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 600 test questions.\n",
                        "Evaluating Q1/600...\n",
                        "Evaluating Q2/600...\n",
                        "Evaluating Q3/600...\n",
                        "Evaluating Q4/600...\n",
                        "Evaluating Q5/600...\n",
                        "Evaluating Q6/600...\n",
                        "Evaluating Q7/600...\n",
                        "Evaluating Q8/600...\n",
                        "Evaluating Q9/600...\n",
                        "Evaluating Q10/600...\n",
                        "Evaluating Q11/600...\n",
                        "Evaluating Q12/600...\n",
                        "Evaluating Q13/600...\n",
                        "Evaluating Q14/600...\n",
                        "Evaluating Q15/600...\n",
                        "Evaluating Q16/600...\n",
                        "Evaluating Q17/600...\n",
                        "Evaluating Q18/600...\n",
                        "Evaluating Q19/600...\n",
                        "Evaluating Q20/600...\n",
                        "Evaluating Q21/600...\n",
                        "Evaluating Q22/600...\n",
                        "Evaluating Q23/600...\n",
                        "Evaluating Q24/600...\n",
                        "Evaluating Q25/600...\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\groq\\_base_client.py:1024\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
                        "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m context_str = format_docs(reranked)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Generation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m rag_response = \u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m rag_time = time.perf_counter() - start\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# --- 2. Evaluator: Intern (Mock) ---\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3157\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3155\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3156\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3157\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3158\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\langchain_core\\runnables\\fallbacks.py:193\u001b[39m, in \u001b[36mRunnableWithFallbacks.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m         output = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrunnable\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m.exceptions_to_handle \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m first_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\langchain_groq\\chat_models.py:621\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    616\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    617\u001b[39m params = {\n\u001b[32m    618\u001b[39m     **params,\n\u001b[32m    619\u001b[39m     **kwargs,\n\u001b[32m    620\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:464\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    245\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    246\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    303\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    304\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    305\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    307\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    462\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\groq\\_base_client.py:1030\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1029\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep_for_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\groq\\_base_client.py:1070\u001b[39m, in \u001b[36mSyncAPIClient._sleep_for_retry\u001b[39m\u001b[34m(self, retries_taken, max_retries, options, response)\u001b[39m\n\u001b[32m   1067\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._calculate_retry_timeout(remaining_retries, options, response.headers \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1068\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "# Load Test Set\n",
                "test_set = []\n",
                "with open(GOLDEN_SET_PATH, 'r') as f:\n",
                "    for line in f:\n",
                "        test_set.append(json.loads(line))\n",
                "\n",
                "print(f\"Loaded {len(test_set)} test questions.\")\n",
                "\n",
                "results = []\n",
                "\n",
                "# Define Judge Model once\n",
                "judge_config = config.copy()\n",
                "judge_config[\"llm_model\"] = \"llama-3.3-70b-versatile\" # Or \"gpt-4-turbo\", \"claude-3-opus\", etc.\n",
                "judge_config[\"llm_provider\"] = \"groq\" # Ensure provider matches\n",
                "\n",
                "try:\n",
                "    judge_bot = get_llm(judge_config)\n",
                "except:\n",
                "    print(\"Warning: Judge model init failed, using default config.\")\n",
                "    judge_bot = get_llm(config)\n",
                "\n",
                "for i, sample in enumerate(test_set):\n",
                "    q = sample['question']\n",
                "    gt = sample['answer']\n",
                "    \n",
                "    print(f\"Evaluating Q{i+1}/{len(test_set)}...\")\n",
                "    \n",
                "    # --- 1. Evaluator: Librarian (RAG) ---\n",
                "    start = time.perf_counter()\n",
                "    # Retrieval\n",
                "    retrieved = hybrid_search(q)\n",
                "    reranked = rerank_results(q, retrieved)\n",
                "    context_str = format_docs(reranked)\n",
                "    # Generation\n",
                "    rag_response = rag_chain.invoke({\"context\": context_str, \"question\": q})\n",
                "    rag_time = time.perf_counter() - start\n",
                "    \n",
                "    # --- 2. Evaluator: Intern (Mock) ---\n",
                "    start = time.perf_counter()\n",
                "    intern_response = intern_chain.invoke({\"question\": q})\n",
                "    intern_time = time.perf_counter() - start\n",
                "    \n",
                "    # --- 3. Scoring ---\n",
                "    \n",
                "    # ROUGE\n",
                "    rag_rouge = calculate_rouge(gt, rag_response)\n",
                "    intern_rouge = calculate_rouge(gt, intern_response)\n",
                "    \n",
                "    # Judge\n",
                "    rag_score, rag_reason = llm_judge(q, gt, rag_response, judge_bot)\n",
                "    intern_score, intern_reason = llm_judge(q, gt, intern_response, judge_bot)\n",
                "    \n",
                "    # Cost (Est.)\n",
                "    rag_cost = calculate_cost(context_str + q, rag_response, config[\"llm_model\"])\n",
                "    intern_cost = calculate_cost(q, intern_response, config[\"llm_model\"])\n",
                "    \n",
                "    results.append({\n",
                "        \"question\": q,\n",
                "        \"ground_truth\": gt,\n",
                "        \"librarian_answer\": rag_response,\n",
                "        \"intern_answer\": intern_response,\n",
                "        \"librarian_time\": rag_time,\n",
                "        \"intern_time\": intern_time,\n",
                "        \"librarian_rouge\": rag_rouge,\n",
                "        \"intern_rouge\": intern_rouge,\n",
                "        \"librarian_score\": rag_score,\n",
                "        \"intern_score\": intern_score,\n",
                "        \"librarian_cost\": rag_cost,\n",
                "        \"intern_cost\": intern_cost,\n",
                "        \"librarian_judge_reason\": rag_reason,\n",
                "        \"intern_judge_reason\": intern_reason\n",
                "    })\n",
                "\n",
                "# Save Results\n",
                "df_res = pd.DataFrame(results)\n",
                "os.makedirs(\"../data/results\", exist_ok=True)\n",
                "df_res.to_json(RESULTS_PATH, orient=\"records\", indent=2)\n",
                "print(f\"Evaluation Complete. Results saved to {RESULTS_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Results & Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary Table\n",
                "summary = df_res[[\n",
                "    \"librarian_time\", \"intern_time\", \n",
                "    \"librarian_rouge\", \"intern_rouge\", \n",
                "    \"librarian_score\", \"intern_score\",\n",
                "    \"librarian_cost\", \"intern_cost\"\n",
                "]].mean()\n",
                "\n",
                "print(\"--- Average Metrics ---\")\n",
                "print(summary)\n",
                "\n",
                "# Detailed View\n",
                "display(df_res[[\"question\", \"librarian_score\", \"intern_score\", \"librarian_rouge\", \"intern_rouge\"]].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Business Cost Analysis\n",
                "**Scenario**: 500 Daily Users, 10 Queries each = 5,000 queries/day."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DAILY_QUERIES = 5000\n",
                "\n",
                "rag_daily_cost = summary[\"librarian_cost\"] * DAILY_QUERIES\n",
                "intern_daily_cost = summary[\"intern_cost\"] * DAILY_QUERIES\n",
                "\n",
                "print(f\"--- ROI / Cost Analysis (Per Day) ---\")\n",
                "print(f\"The Librarian (RAG) Cost: ${rag_daily_cost:.2f}\")\n",
                "print(f\"The Intern (Fine-Tuned) Cost: ${intern_daily_cost:.2f}\")\n",
                "\n",
                "diff = rag_daily_cost - intern_daily_cost\n",
                "if diff > 0:\n",
                "    print(f\"Fine-Tuned Model saves ${diff:.2f} per day (${diff*30:.2f}/month).\")\n",
                "else:\n",
                "    print(f\"RAG System saves ${abs(diff):.2f} per day (${abs(diff)*30:.2f}/month).\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "sahas",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
