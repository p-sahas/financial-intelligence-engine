{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 4: The Showdown (Evaluation Arena)\n",
                "\n",
                "**Objective:** Compare \"The Librarian\" (RAG System) vs. \"The Intern\" (Fine-Tuned Model) on the Golden Test Set.\n",
                "\n",
                "**Metrics:**\n",
                "1.  **ROUGE-L**: Measures text overlap (Precision, Recall, F1) against Ground Truth.\n",
                "2.  **LLM-as-a-Judge**: Uses a superior model (e.g., GPT-4o) to score answer quality (1-5) and reasoning.\n",
                "3.  **Latency**: Time taken to generate the answer.\n",
                "4.  **Cost**: Estimated cost per 1k queries.\n",
                "\n",
                "**Note:** Since the fine-tuning step was skipped/mocked, \"The Intern\" evaluation will use the **Base Model** (Llama-3-8B) as a proxy to demonstrate the pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies (if needed)\n",
                "# !pip install -q rouge_score weave weaviate-client langchain langchain-community langchain-huggingface sentence-transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Config Loaded (partial). Testing on: ../data/processed/golden_test_set.jsonl\n"
                    ]
                }
            ],
            "source": [
                "# 2. Imports & Configuration\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "import yaml\n",
                "import pandas as pd\n",
                "import weaviate\n",
                "from rouge_score import rouge_scorer\n",
                "\n",
                "# Add src to path\n",
                "import sys\n",
                "sys.path.append(os.path.abspath(\"../src\"))\n",
                "\n",
                "from services.llm_services import get_llm\n",
                "from utils.cost_tracker import get_token_count, PRICING\n",
                "\n",
                "# Load Config (safe)\n",
                "config = {}\n",
                "try:\n",
                "    with open(\"../src/config/config.yaml\", \"r\") as f:\n",
                "        config = yaml.safe_load(f) or {}\n",
                "except FileNotFoundError:\n",
                "    print(\"Warning: ../src/config/config.yaml not found. Using defaults.\")\n",
                "except Exception as e:\n",
                "    print(f\"Warning: Error loading config: {e}. Using defaults.\")\n",
                "\n",
                "# Paths with fallbacks to avoid KeyError if 'data' is missing in config\n",
                "data_cfg = config.get(\"data\", {})\n",
                "GOLDEN_SET_PATH = data_cfg.get(\"golden_test_set_path\", '../data/processed/golden_test_set.jsonl')\n",
                "RESULTS_PATH = data_cfg.get(\"eval_results_path\", '../data/results/rag_evaluation_results.json')\n",
                "\n",
                "print(f\"Config Loaded (partial). Testing on: {GOLDEN_SET_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Metric Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_rouge(reference, candidate):\n",
                "    \"\"\"Calculates ROUGE-L score.\"\"\"\n",
                "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
                "    scores = scorer.score(reference, candidate)\n",
                "    return scores['rougeL'].fmeasure\n",
                "\n",
                "def calculate_cost(input_text, output_text, model_name):\n",
                "    \"\"\"Estimates cost based on token counts and PRICING.\"\"\"\n",
                "    in_tokens = get_token_count(input_text)\n",
                "    out_tokens = get_token_count(output_text)\n",
                "    \n",
                "    # Normalize model name key (handle variations)\n",
                "    pricing = PRICING.get(model_name, PRICING.get(\"openai/gpt-4o-mini\"))\n",
                "    \n",
                "    cost = (in_tokens / 1_000_000) * pricing[\"input\"] + (out_tokens / 1_000_000) * pricing[\"output\"]\n",
                "    return cost\n",
                "\n",
                "import re\n",
                "import time\n",
                "\n",
                "def llm_judge(question, ground_truth, answer, judge_model=None):\n",
                "    \"\"\"Uses an LLM to grade the answer 1-5.\"\"\"\n",
                "    if judge_model is None:\n",
                "        # Initialize a strong judge model (e.g. GPT-4o or similar)\n",
                "        try:\n",
                "            # Use 'google' provider if 'gemini' (or vice-versa) to ensure fallback options\n",
                "            judge_config = config.copy()\n",
                "            judge_config[\"llm_model\"] = \"gpt-4o\" # Force strong model if available\n",
                "            judge_model = get_llm(judge_config)\n",
                "        except Exception:\n",
                "            print(\"Warning: Judge model init failed, using default config.\")\n",
                "            judge_model = get_llm(config)\n",
                "\n",
                "    prompt_template = \"\"\"\n",
                "    You are an impartial judge evaluating a financial analyst's answer.\n",
                "    \n",
                "    Question: {question}\n",
                "    Ground Truth: {ground_truth}\n",
                "    Student Answer: {answer}\n",
                "    \n",
                "    Evaluate the Student Answer based on accuracy, completeness, and tone compared to the Ground Truth.\n",
                "    Output ONLY a valid JSON object with two keys:\n",
                "    - \"score\": an integer 1-5 (1=bad, 5=excellent)\n",
                "    - \"reasoning\": a brief explanation.\n",
                "    Do NOT output markdown formatting, just the raw JSON.\n",
                "    \"\"\"\n",
                "    \n",
                "    prompt = prompt_template.format(question=question, ground_truth=ground_truth, answer=answer)\n",
                "    \n",
                "    # Initialize content OUTSIDE the try/loop to avoid UnboundLocalError\n",
                "    content = \"\"\n",
                "    max_retries = 3\n",
                "    \n",
                "    for attempt in range(max_retries):\n",
                "        try:\n",
                "            response = judge_model.invoke(prompt)\n",
                "            content = response.content.strip()\n",
                "            \n",
                "            # Enhanced JSON extraction for Gemini/other models\n",
                "            match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
                "            if match:\n",
                "                content = match.group(0)\n",
                "            \n",
                "            content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
                "            \n",
                "            result = json.loads(content)\n",
                "            return result.get(\"score\", 0), result.get(\"reasoning\", \"Parse Error\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            error_str = str(e)\n",
                "            is_rate_limit = \"429\" in error_str or \"RESOURCE_EXHAUSTED\" in error_str\n",
                "            \n",
                "            if is_rate_limit:\n",
                "                wait_time = 20 * (attempt + 1) # Progressive backoff: 20s, 40s, 60s\n",
                "                print(f\"Judge Rate Limit (429). Waiting {wait_time}s... (Attempt {attempt+1}/{max_retries})\")\n",
                "                time.sleep(wait_time)\n",
                "                continue  # Retry\n",
                "            else:\n",
                "                # Safeguard print against uninitialized content\n",
                "                debug_content = content[:100] if content else \"<Empty/Failed>\"\n",
                "                print(f\"Judge Error: {e}\\nContent: {debug_content}...\")\n",
                "                return 0, \"Error\"\n",
                "                \n",
                "    return 0, \"Error - Rate Limit Exceeded\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. System 1: \"The Librarian\" (RAG Setup)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:From c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
                        "\n",
                        "1. Trying Local Docker...\n",
                        " Connected to Local Docker!\n",
                        "Weaviate Ready: True\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Sahas Induwara\\.conda\\envs\\sahas\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from sentence_transformers import CrossEncoder\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "\n",
                "# --- 1. Connect to Weaviate ---\n",
                "client = None\n",
                "\n",
                "# 1. Try Local Docker (Preferred for Windows)\n",
                "print(\"1. Trying Local Docker...\")\n",
                "try:\n",
                "    client = weaviate.connect_to_local()\n",
                "    if client.is_ready():\n",
                "        print(\" Connected to Local Docker!\")\n",
                "except Exception as e:\n",
                "    print(f\" Local connection failed: {e}\")\n",
                "    client = None\n",
                "\n",
                "# 2. Try Cloud (WCS) if Local failed\n",
                "if not client:\n",
                "    print(\"2. Trying Weaviate Cloud...\")\n",
                "    try:\n",
                "        # Fix: Safely get config, fallback to env vars\n",
                "        vectordb_cfg = config.get(\"vector_db\", config.get(\"vectordb\", {}))\n",
                "        wcs_url = vectordb_cfg.get(\"wcs_url\") or os.environ.get(\"WEAVIATE_URL\")\n",
                "        wcs_api_key = vectordb_cfg.get(\"wcs_api_key\") or os.environ.get(\"WEAVIATE_API_KEY\")\n",
                "        \n",
                "        if wcs_url and wcs_api_key:\n",
                "            client = weaviate.connect_to_wcs(\n",
                "                cluster_url=wcs_url,\n",
                "                auth_credentials=weaviate.auth.AuthApiKey(wcs_api_key)\n",
                "            )\n",
                "            print(\" Connected to Weaviate Cloud!\")\n",
                "        else:\n",
                "            print(\" No Cloud credentials found.\")\n",
                "    except Exception as e:\n",
                "        print(f\" Cloud connection failed: {e}\")\n",
                "\n",
                "if not client or not client.is_ready():\n",
                "    print(\"CRITICAL: Weaviate connection failed.\")\n",
                "else:\n",
                "    print(f\"Weaviate Ready: {client.is_ready()}\")\n",
                "\n",
                "# --- 2. Embeddings & Reranker ---\n",
                "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
                "\n",
                "# --- 3. Retrieval Functions ---\n",
                "def hybrid_search(query, limit=20):\n",
                "    if not client:\n",
                "        raise RuntimeError(\"Weaviate client is not connected.\")\n",
                "    \n",
                "    # FIX: Get collection name from config\n",
                "    vectordb_cfg = config.get(\"vector_db\", config.get(\"vectordb\", {}))\n",
                "    collection_name = vectordb_cfg.get(\"collection_name\", \"FinancialReport\")\n",
                "    \n",
                "    collection = client.collections.get(collection_name)\n",
                "    query_vector = embedding_model.embed_query(query)\n",
                "    response = collection.query.hybrid(\n",
                "        query=query,\n",
                "        vector=query_vector,\n",
                "        alpha=0.5,\n",
                "        limit=limit,\n",
                "        return_metadata=weaviate.classes.query.MetadataQuery(score=True)\n",
                "    )\n",
                "    results = []\n",
                "    for o in response.objects:\n",
                "        res = o.properties\n",
                "        res['score'] = o.metadata.score\n",
                "        results.append(res)\n",
                "    return results\n",
                "\n",
                "def rerank_results(query, retrieved_docs, top_k=5):\n",
                "    if not retrieved_docs: return []\n",
                "    pairs = [[query, doc['text']] for doc in retrieved_docs]\n",
                "    scores = reranker.predict(pairs)\n",
                "    for i, doc in enumerate(retrieved_docs):\n",
                "        doc['rerank_score'] = float(scores[i])\n",
                "    return sorted(retrieved_docs, key=lambda x: x['rerank_score'], reverse=True)[:top_k]\n",
                "\n",
                "def format_docs(docs):\n",
                "    return \"\\n\\n\".join([f\"[Source: Page {d.get('page_number', '?')}] {d['text']}\" for d in docs])\n",
                "\n",
                "# --- 4. RAG Chain ---\n",
                "rag_template = \"\"\"\n",
                "You are a specialized financial analyst assistant.\n",
                "Use the following context to answer the user's question accurately.\n",
                "If the answer is not in the context, say \"I don't have enough information.\"\n",
                "Keep answers professional and concise.\n",
                "\n",
                "Context:\n",
                "{context}\n",
                "\n",
                "Question: {question}\n",
                "Answer:\n",
                "\"\"\"\n",
                "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
                "rag_llm = get_llm(config) # Uses default llm_model from config\n",
                "rag_chain = rag_prompt | rag_llm | StrOutputParser()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. System 2: \"The Intern\" (Mock Fine-Tuned Model)\n",
                "Since we didn't perform the physical fine-tuning, we will use the **Base Model** directly as a proxy. Realistically, this would be the `PeftModel` loaded from disk."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "intern_template = \"\"\"\n",
                "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "You are a financial analyst specializing in the 2024 Annual Report. Answer strictly based on your internal knowledge and the following question.\n",
                "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                "{question}\n",
                "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
                "\"\"\"\n",
                "\n",
                "intern_prompt = ChatPromptTemplate.from_template(intern_template)\n",
                "intern_llm = get_llm(config)\n",
                "intern_chain = intern_prompt | intern_llm | StrOutputParser()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Run Evaluation Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 600 test questions.\n",
                        "Evaluating Q1/600...\n",
                        "Evaluating Q2/600...\n",
                        "Evaluating Q3/600...\n",
                        "Evaluating Q4/600...\n",
                        "Evaluating Q5/600...\n",
                        "Evaluating Q6/600...\n",
                        "Evaluating Q7/600...\n",
                        "Evaluating Q8/600...\n",
                        "Evaluating Q9/600...\n",
                        "Evaluating Q10/600...\n",
                        "Evaluating Q11/600...\n",
                        "Evaluating Q12/600...\n",
                        "Evaluating Q13/600...\n",
                        "Evaluating Q14/600...\n",
                        "Evaluating Q15/600...\n",
                        "Evaluation Complete. Results saved to ../data/results/rag_evaluation_results.json\n"
                    ]
                }
            ],
            "source": [
                "# Load Test Set\n",
                "test_set = []\n",
                "with open(GOLDEN_SET_PATH, 'r') as f:\n",
                "    for line in f:\n",
                "        test_set.append(json.loads(line))\n",
                "\n",
                "print(f\"Loaded {len(test_set)} test questions.\")\n",
                "\n",
                "results = []\n",
                "\n",
                "# Define Judge Model once\n",
                "judge_config = config.copy()\n",
                "judge_config[\"llm_model\"] = \"gpt-4o-mini\" \n",
                "judge_config[\"llm_provider\"] = \"openrouter\" # Ensure provider matches\n",
                "\n",
                "try:\n",
                "    judge_bot = get_llm(judge_config)\n",
                "except:\n",
                "    print(\"Warning: Judge model init failed, using default config.\")\n",
                "    judge_bot = get_llm(config)\n",
                "\n",
                "for i, sample in enumerate(test_set[:15]): # Limit to 15 for speed/cost\n",
                "    q = sample['question']\n",
                "    gt = sample['answer']\n",
                "    \n",
                "    print(f\"Evaluating Q{i+1}/{len(test_set)}...\")\n",
                "    \n",
                "    # --- 1. Evaluator: Librarian (RAG) ---\n",
                "    start = time.perf_counter()\n",
                "    # Retrieval\n",
                "    retrieved = hybrid_search(q)\n",
                "    reranked = rerank_results(q, retrieved)\n",
                "    context_str = format_docs(reranked)\n",
                "    # Generation\n",
                "    rag_response = rag_chain.invoke({\"context\": context_str, \"question\": q})\n",
                "    rag_response = rag_chain.invoke({\"context\": context_str, \"question\": q})\n",
                "    time.sleep(10) # 10s delay (15 RPM limit -> ~16s total per q)\n",
                "    rag_time = time.perf_counter() - start\n",
                "    \n",
                "    # --- 2. Evaluator: Intern (Mock) ---\n",
                "    start = time.perf_counter()\n",
                "    intern_response = intern_chain.invoke({\"question\": q})\n",
                "    intern_time = time.perf_counter() - start\n",
                "    \n",
                "    # --- 3. Scoring ---\n",
                "    \n",
                "    # ROUGE\n",
                "    rag_rouge = calculate_rouge(gt, rag_response)\n",
                "    intern_rouge = calculate_rouge(gt, intern_response)\n",
                "    \n",
                "    # Judge\n",
                "    rag_score, rag_reason = llm_judge(q, gt, rag_response, judge_bot)\n",
                "    intern_score, intern_reason = llm_judge(q, gt, intern_response, judge_bot)\n",
                "    \n",
                "    # Cost (Est.)\n",
                "    rag_cost = calculate_cost(context_str + q, rag_response, config[\"llm_model\"])\n",
                "    intern_cost = calculate_cost(q, intern_response, config[\"llm_model\"])\n",
                "    \n",
                "    results.append({\n",
                "        \"question\": q,\n",
                "        \"ground_truth\": gt,\n",
                "        \"librarian_answer\": rag_response,\n",
                "        \"intern_answer\": intern_response,\n",
                "        \"librarian_time\": rag_time,\n",
                "        \"intern_time\": intern_time,\n",
                "        \"librarian_rouge\": rag_rouge,\n",
                "        \"intern_rouge\": intern_rouge,\n",
                "        \"librarian_score\": rag_score,\n",
                "        \"intern_score\": intern_score,\n",
                "        \"librarian_cost\": rag_cost,\n",
                "        \"intern_cost\": intern_cost,\n",
                "        \"librarian_judge_reason\": rag_reason,\n",
                "        \"intern_judge_reason\": intern_reason\n",
                "    })\n",
                "    time.sleep(10) # 10s delay (15 RPM limit -> ~16s total per q)\n",
                "\n",
                "# Save Results\n",
                "df_res = pd.DataFrame(results)\n",
                "os.makedirs(\"../data/results\", exist_ok=True)\n",
                "df_res.to_json(RESULTS_PATH, orient=\"records\", indent=2)\n",
                "print(f\"Evaluation Complete. Results saved to {RESULTS_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Results & Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Average Metrics ---\n",
                        "librarian_time     13.757364\n",
                        "intern_time         5.000468\n",
                        "librarian_rouge     0.400217\n",
                        "intern_rouge        0.164422\n",
                        "librarian_score     4.133333\n",
                        "intern_score        4.000000\n",
                        "librarian_cost      0.000196\n",
                        "intern_cost         0.000139\n",
                        "dtype: float64\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>question</th>\n",
                            "      <th>librarian_score</th>\n",
                            "      <th>intern_score</th>\n",
                            "      <th>librarian_rouge</th>\n",
                            "      <th>intern_rouge</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>How does the tone of the text reflect the comp...</td>\n",
                            "      <td>1</td>\n",
                            "      <td>3</td>\n",
                            "      <td>0.000000</td>\n",
                            "      <td>0.162963</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>What types of legislative proposals are curren...</td>\n",
                            "      <td>3</td>\n",
                            "      <td>5</td>\n",
                            "      <td>0.181818</td>\n",
                            "      <td>0.013746</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>What strategic goal can be inferred from the s...</td>\n",
                            "      <td>4</td>\n",
                            "      <td>4</td>\n",
                            "      <td>0.181818</td>\n",
                            "      <td>0.023529</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>What tone is used when discussing the potentia...</td>\n",
                            "      <td>5</td>\n",
                            "      <td>5</td>\n",
                            "      <td>0.517241</td>\n",
                            "      <td>0.236559</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>What could be the potential consequences for t...</td>\n",
                            "      <td>5</td>\n",
                            "      <td>5</td>\n",
                            "      <td>0.598425</td>\n",
                            "      <td>0.123324</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                            question  librarian_score  \\\n",
                            "0  How does the tone of the text reflect the comp...                1   \n",
                            "1  What types of legislative proposals are curren...                3   \n",
                            "2  What strategic goal can be inferred from the s...                4   \n",
                            "3  What tone is used when discussing the potentia...                5   \n",
                            "4  What could be the potential consequences for t...                5   \n",
                            "\n",
                            "   intern_score  librarian_rouge  intern_rouge  \n",
                            "0             3         0.000000      0.162963  \n",
                            "1             5         0.181818      0.013746  \n",
                            "2             4         0.181818      0.023529  \n",
                            "3             5         0.517241      0.236559  \n",
                            "4             5         0.598425      0.123324  "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Summary Table\n",
                "summary = df_res[[\n",
                "    \"librarian_time\", \"intern_time\", \n",
                "    \"librarian_rouge\", \"intern_rouge\", \n",
                "    \"librarian_score\", \"intern_score\",\n",
                "    \"librarian_cost\", \"intern_cost\"\n",
                "]].mean()\n",
                "\n",
                "print(\"--- Average Metrics ---\")\n",
                "print(summary)\n",
                "\n",
                "# Detailed View\n",
                "display(df_res[[\"question\", \"librarian_score\", \"intern_score\", \"librarian_rouge\", \"intern_rouge\"]].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Business Cost Analysis\n",
                "**Scenario**: 500 Daily Users, 10 Queries each = 5,000 queries/day."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- ROI / Cost Analysis (Per Day) ---\n",
                        "The Librarian (RAG) Cost: $0.98\n",
                        "The Intern (Fine-Tuned) Cost: $0.70\n",
                        "Fine-Tuned Model saves $0.29 per day ($8.58/month).\n"
                    ]
                }
            ],
            "source": [
                "DAILY_QUERIES = 5000\n",
                "\n",
                "rag_daily_cost = summary[\"librarian_cost\"] * DAILY_QUERIES\n",
                "intern_daily_cost = summary[\"intern_cost\"] * DAILY_QUERIES\n",
                "\n",
                "print(f\"--- ROI / Cost Analysis (Per Day) ---\")\n",
                "print(f\"The Librarian (RAG) Cost: ${rag_daily_cost:.2f}\")\n",
                "print(f\"The Intern (Fine-Tuned) Cost: ${intern_daily_cost:.2f}\")\n",
                "\n",
                "diff = rag_daily_cost - intern_daily_cost\n",
                "if diff > 0:\n",
                "    print(f\"Fine-Tuned Model saves ${diff:.2f} per day (${diff*30:.2f}/month).\")\n",
                "else:\n",
                "    print(f\"RAG System saves ${abs(diff):.2f} per day (${abs(diff)*30:.2f}/month).\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "sahas",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
